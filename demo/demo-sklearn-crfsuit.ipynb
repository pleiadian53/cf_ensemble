{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['esp.testa', 'esp.testb', 'esp.train', 'ned.testa', 'ned.testb', 'ned.train']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.conll2002.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.98 s, sys: 366 ms, total: 7.35 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Melbourne', 'NP', 'B-LOC'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('Australia', 'NP', 'B-LOC'),\n",
       " (')', 'Fpt', 'O'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('25', 'Z', 'O'),\n",
       " ('may', 'NC', 'O'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('EFE', 'NC', 'B-ORG'),\n",
       " (')', 'Fpt', 'O'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features \n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],        \n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "        \n",
    "    features['num-1'] = np.log(3.99 * ((i+1) **2))\n",
    "    features['num-2'] = 0.28 * i             \n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': 1.0,\n",
       " 'word.lower()': 'melbourne',\n",
       " 'word[-3:]': 'rne',\n",
       " 'word[-2:]': 'ne',\n",
       " 'word.isupper()': False,\n",
       " 'word.istitle()': True,\n",
       " 'word.isdigit()': False,\n",
       " 'postag': 'NP',\n",
       " 'postag[:2]': 'NP',\n",
       " 'BOS': True,\n",
       " '+1:word.lower()': '(',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:postag': 'Fpa',\n",
       " '+1:postag[:2]': 'Fp',\n",
       " 'num-1': 1.3837912309017721,\n",
       " 'num-2': 0.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a feature set is a dictionary\n",
    "\n",
    "sent2features(train_sents[0])[0] # sequence of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> dim(X_train): 8323, dim(X_test):1517 \n",
      "> dtype | type(X_train[0]): <class 'list'>, type(X_train[0][0]): <class 'dict'>\n",
      "> y_test: 1517\n",
      "CPU times: user 5.68 s, sys: 425 ms, total: 6.11 s\n",
      "Wall time: 9.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]   # list of lists\n",
    "\n",
    "print(\"> dim(X_train): {}, dim(X_test):{} \".format(len(X_train), len(X_test)))\n",
    "print(\"> dtype | type(X_train[0]): {}, type(X_train[0][0]): {}\".format(type(X_train[0]), type(X_train[0][0])))\n",
    "print(\"> y_test: {}\".format(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> X_train[0][0]: {'bias': 1.0, 'word.lower()': 'melbourne', 'word[-3:]': 'rne', 'word[-2:]': 'ne', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'NP', 'postag[:2]': 'NP', 'BOS': True, '+1:word.lower()': '(', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'Fpa', '+1:postag[:2]': 'Fp', 'num-1': 1.3837912309017721, 'num-2': 0.0}\n",
      "> X_train[0]: [{'bias': 1.0, 'word.lower()': 'melbourne', 'word[-3:]': 'rne', 'word[-2:]': 'ne', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'NP', 'postag[:2]': 'NP', 'BOS': True, '+1:word.lower()': '(', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'Fpa', '+1:postag[:2]': 'Fp', 'num-1': 1.3837912309017721, 'num-2': 0.0}, {'bias': 1.0, 'word.lower()': '(', 'word[-3:]': '(', 'word[-2:]': '(', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'Fpa', 'postag[:2]': 'Fp', '-1:word.lower()': 'melbourne', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'NP', '-1:postag[:2]': 'NP', '+1:word.lower()': 'australia', '+1:word.istitle()': True, '+1:word.isupper()': False, '+1:postag': 'NP', '+1:postag[:2]': 'NP', 'num-1': 2.7700855920216627, 'num-2': 0.28}, {'bias': 1.0, 'word.lower()': 'australia', 'word[-3:]': 'lia', 'word[-2:]': 'ia', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'NP', 'postag[:2]': 'NP', '-1:word.lower()': '(', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'Fpa', '-1:postag[:2]': 'Fp', '+1:word.lower()': ')', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'Fpt', '+1:postag[:2]': 'Fp', 'num-1': 3.5810158082379915, 'num-2': 0.56}, {'bias': 1.0, 'word.lower()': ')', 'word[-3:]': ')', 'word[-2:]': ')', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'Fpt', 'postag[:2]': 'Fp', '-1:word.lower()': 'australia', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'NP', '-1:postag[:2]': 'NP', '+1:word.lower()': ',', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'Fc', '+1:postag[:2]': 'Fc', 'num-1': 4.1563799531415535, 'num-2': 0.8400000000000001}, {'bias': 1.0, 'word.lower()': ',', 'word[-3:]': ',', 'word[-2:]': ',', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'Fc', 'postag[:2]': 'Fc', '-1:word.lower()': ')', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'Fpt', '-1:postag[:2]': 'Fp', '+1:word.lower()': '25', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'Z', '+1:postag[:2]': 'Z', 'num-1': 4.602667055769973, 'num-2': 1.12}, {'bias': 1.0, 'word.lower()': '25', 'word[-3:]': '25', 'word[-2:]': '25', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': True, 'postag': 'Z', 'postag[:2]': 'Z', '-1:word.lower()': ',', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'Fc', '-1:postag[:2]': 'Fc', '+1:word.lower()': 'may', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'NC', '+1:postag[:2]': 'NC', 'num-1': 4.967310169357882, 'num-2': 1.4000000000000001}, {'bias': 1.0, 'word.lower()': 'may', 'word[-3:]': 'may', 'word[-2:]': 'ay', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'NC', 'postag[:2]': 'NC', '-1:word.lower()': '25', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'Z', '-1:postag[:2]': 'Z', '+1:word.lower()': '(', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'Fpa', '+1:postag[:2]': 'Fp', 'num-1': 5.275611529012399, 'num-2': 1.6800000000000002}, {'bias': 1.0, 'word.lower()': '(', 'word[-3:]': '(', 'word[-2:]': '(', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'Fpa', 'postag[:2]': 'Fp', '-1:word.lower()': 'may', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'NC', '-1:postag[:2]': 'NC', '+1:word.lower()': 'efe', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'NC', '+1:postag[:2]': 'NC', 'num-1': 5.542674314261444, 'num-2': 1.9600000000000002}, {'bias': 1.0, 'word.lower()': 'efe', 'word[-3:]': 'EFE', 'word[-2:]': 'FE', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'NC', 'postag[:2]': 'NC', '-1:word.lower()': '(', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'Fpa', '-1:postag[:2]': 'Fp', '+1:word.lower()': ')', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'Fpt', '+1:postag[:2]': 'Fp', 'num-1': 5.778240385574211, 'num-2': 2.24}, {'bias': 1.0, 'word.lower()': ')', 'word[-3:]': ')', 'word[-2:]': ')', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'Fpt', 'postag[:2]': 'Fp', '-1:word.lower()': 'efe', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'NC', '-1:postag[:2]': 'NC', '+1:word.lower()': '.', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'Fp', '+1:postag[:2]': 'Fp', 'num-1': 5.988961416889864, 'num-2': 2.5200000000000005}, {'bias': 1.0, 'word.lower()': '.', 'word[-3:]': '.', 'word[-2:]': '.', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'Fp', 'postag[:2]': 'Fp', '-1:word.lower()': ')', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'Fpt', '-1:postag[:2]': 'Fp', 'EOS': True, 'num-1': 6.179581776498513, 'num-2': 2.8000000000000003}]\n",
      "> y_train[0]: ['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
      "CPU times: user 1min 56s, sys: 1.88 s, total: 1min 58s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "print(\"> X_train[0][0]: {}\".format(X_train[0][0]))\n",
    "print(\"> X_train[0]: {}\".format(X_train[0]))  # list of dictionary\n",
    "print(\"> y_train[0]: {}\".format(y_train[0]))  # list of strings\n",
    "\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-ORG', 'B-PER', 'I-PER', 'B-MISC', 'I-ORG', 'I-LOC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('bias', 'B-LOC'): -0.298796,\n",
       " ('bias', 'O'): 1.532135,\n",
       " ('bias', 'B-ORG'): -0.035727,\n",
       " ('bias', 'B-PER'): -0.207561,\n",
       " ('bias', 'I-PER'): -0.702335,\n",
       " ('bias', 'B-MISC'): -0.064028,\n",
       " ('bias', 'I-ORG'): -0.408596,\n",
       " ('bias', 'I-LOC'): -0.017751,\n",
       " ('bias', 'I-MISC'): -0.083637,\n",
       " ('word.lower():melbourne', 'B-LOC'): 1.113998,\n",
       " ('word.lower():melbourne', 'I-MISC'): 0.407021,\n",
       " ('word[-3:]:rne', 'B-LOC'): 0.623061,\n",
       " ('word[-3:]:rne', 'B-ORG'): 0.000644,\n",
       " ('word[-3:]:rne', 'I-MISC'): 0.376198,\n",
       " ('word[-2:]:ne', 'B-LOC'): 6.3e-05,\n",
       " ('word[-2:]:ne', 'O'): 0.695846,\n",
       " ('word[-2:]:ne', 'B-ORG'): 0.232472,\n",
       " ('word[-2:]:ne', 'B-PER'): 0.683416,\n",
       " ('word[-2:]:ne', 'I-PER'): 0.003849,\n",
       " ('word[-2:]:ne', 'I-ORG'): -0.020744,\n",
       " ('word[-2:]:ne', 'I-MISC'): -0.025966,\n",
       " ('word.isupper()', 'B-LOC'): 0.304402,\n",
       " ('word.isupper()', 'O'): -5.749025,\n",
       " ('word.isupper()', 'B-ORG'): 2.659295,\n",
       " ('word.isupper()', 'B-PER'): 0.270291,\n",
       " ('word.isupper()', 'I-PER'): -0.176889,\n",
       " ('word.isupper()', 'B-MISC'): 2.25353,\n",
       " ('word.isupper()', 'I-ORG'): 0.449349,\n",
       " ('word.isupper()', 'I-LOC'): -0.88197,\n",
       " ('word.isupper()', 'I-MISC'): -0.082333,\n",
       " ('word.istitle()', 'B-LOC'): 1.368938,\n",
       " ('word.istitle()', 'O'): -8.35175,\n",
       " ('word.istitle()', 'B-ORG'): 0.427883,\n",
       " ('word.istitle()', 'B-PER'): 1.336367,\n",
       " ('word.istitle()', 'I-PER'): 0.54929,\n",
       " ('word.istitle()', 'B-MISC'): 0.488053,\n",
       " ('word.istitle()', 'I-ORG'): 0.283431,\n",
       " ('word.istitle()', 'I-LOC'): 0.171484,\n",
       " ('word.istitle()', 'I-MISC'): -0.168541,\n",
       " ('word.isdigit()', 'O'): -0.221899,\n",
       " ('word.isdigit()', 'B-ORG'): 1.221146,\n",
       " ('word.isdigit()', 'B-PER'): -0.281832,\n",
       " ('word.isdigit()', 'I-PER'): -0.137499,\n",
       " ('word.isdigit()', 'B-MISC'): -0.004182,\n",
       " ('word.isdigit()', 'I-ORG'): 0.686547,\n",
       " ('word.isdigit()', 'I-LOC'): -0.017163,\n",
       " ('word.isdigit()', 'I-MISC'): 1.101579,\n",
       " ('postag:NP', 'B-LOC'): 0.292478,\n",
       " ('postag:NP', 'O'): -1.333111,\n",
       " ('postag:NP', 'B-ORG'): 0.669628,\n",
       " ('postag:NP', 'B-PER'): 0.448072,\n",
       " ('postag:NP', 'I-PER'): -0.45368,\n",
       " ('postag:NP', 'B-MISC'): 0.028969,\n",
       " ('postag:NP', 'I-ORG'): -0.490643,\n",
       " ('postag[:2]:NP', 'B-LOC'): 0.292478,\n",
       " ('postag[:2]:NP', 'O'): -1.333111,\n",
       " ('postag[:2]:NP', 'B-ORG'): 0.669628,\n",
       " ('postag[:2]:NP', 'B-PER'): 0.448072,\n",
       " ('postag[:2]:NP', 'I-PER'): -0.45368,\n",
       " ('postag[:2]:NP', 'B-MISC'): 0.028969,\n",
       " ('postag[:2]:NP', 'I-ORG'): -0.490643,\n",
       " ('BOS', 'B-LOC'): 0.863542,\n",
       " ('BOS', 'O'): 6.29895,\n",
       " ('BOS', 'B-ORG'): -0.311949,\n",
       " ('BOS', 'B-PER'): 0.972106,\n",
       " ('BOS', 'I-LOC'): -5.17388,\n",
       " ('+1:word.lower():(', 'B-LOC'): 0.612888,\n",
       " ('+1:word.lower():(', 'O'): -0.441123,\n",
       " ('+1:word.lower():(', 'B-ORG'): -0.208028,\n",
       " ('+1:word.lower():(', 'B-PER'): 0.125666,\n",
       " ('+1:word.lower():(', 'I-PER'): 0.112014,\n",
       " ('+1:word.lower():(', 'B-MISC'): -0.04918,\n",
       " ('+1:word.lower():(', 'I-ORG'): 0.516232,\n",
       " ('+1:word.lower():(', 'I-LOC'): 0.220286,\n",
       " ('+1:word.lower():(', 'I-MISC'): -0.016976,\n",
       " ('+1:word.istitle()', 'B-LOC'): 0.035402,\n",
       " ('+1:word.istitle()', 'O'): 0.366936,\n",
       " ('+1:word.istitle()', 'B-ORG'): 0.027122,\n",
       " ('+1:word.istitle()', 'B-PER'): 0.686241,\n",
       " ('+1:word.istitle()', 'I-PER'): 0.46396,\n",
       " ('+1:word.istitle()', 'B-MISC'): 0.091135,\n",
       " ('+1:word.istitle()', 'I-ORG'): 0.210484,\n",
       " ('+1:word.istitle()', 'I-LOC'): 0.172794,\n",
       " ('+1:word.istitle()', 'I-MISC'): -0.041689,\n",
       " ('+1:word.isupper()', 'B-LOC'): -0.735472,\n",
       " ('+1:word.isupper()', 'O'): 0.437797,\n",
       " ('+1:word.isupper()', 'B-ORG'): -0.150202,\n",
       " ('+1:word.isupper()', 'B-PER'): 0.158398,\n",
       " ('+1:word.isupper()', 'I-PER'): -0.140102,\n",
       " ('+1:word.isupper()', 'B-MISC'): 0.14605,\n",
       " ('+1:word.isupper()', 'I-ORG'): -0.626642,\n",
       " ('+1:word.isupper()', 'I-LOC'): -1.259407,\n",
       " ('+1:word.isupper()', 'I-MISC'): -0.313019,\n",
       " ('+1:postag:Fpa', 'B-LOC'): 0.612888,\n",
       " ('+1:postag:Fpa', 'O'): -0.441123,\n",
       " ('+1:postag:Fpa', 'B-ORG'): -0.208028,\n",
       " ('+1:postag:Fpa', 'B-PER'): 0.125666,\n",
       " ('+1:postag:Fpa', 'I-PER'): 0.112014,\n",
       " ('+1:postag:Fpa', 'B-MISC'): -0.04918,\n",
       " ('+1:postag:Fpa', 'I-ORG'): 0.516232,\n",
       " ('+1:postag:Fpa', 'I-LOC'): 0.220286,\n",
       " ('+1:postag:Fpa', 'I-MISC'): -0.016976,\n",
       " ('+1:postag[:2]:Fp', 'B-LOC'): 0.156477,\n",
       " ('+1:postag[:2]:Fp', 'O'): 0.14301,\n",
       " ('+1:postag[:2]:Fp', 'B-ORG'): -0.00052,\n",
       " ('+1:postag[:2]:Fp', 'B-PER'): 0.166693,\n",
       " ('+1:postag[:2]:Fp', 'I-PER'): 0.270003,\n",
       " ('+1:postag[:2]:Fp', 'B-MISC'): 0.27419,\n",
       " ('+1:postag[:2]:Fp', 'I-ORG'): -0.01808,\n",
       " ('+1:postag[:2]:Fp', 'I-LOC'): 0.073777,\n",
       " ('+1:postag[:2]:Fp', 'I-MISC'): -0.146072,\n",
       " ('num-1', 'B-LOC'): -0.090733,\n",
       " ('num-1', 'O'): 0.11831,\n",
       " ('num-1', 'B-ORG'): -0.129715,\n",
       " ('num-1', 'B-PER'): -0.108866,\n",
       " ('num-1', 'I-PER'): 0.056878,\n",
       " ('num-1', 'B-MISC'): -0.172912,\n",
       " ('num-1', 'I-ORG'): 0.063139,\n",
       " ('num-1', 'I-LOC'): 0.160951,\n",
       " ('num-1', 'I-MISC'): 0.222644,\n",
       " ('num-2', 'B-LOC'): 1.223207,\n",
       " ('num-2', 'O'): 1.204476,\n",
       " ('num-2', 'B-ORG'): 1.233175,\n",
       " ('num-2', 'B-PER'): 1.208559,\n",
       " ('num-2', 'I-PER'): 1.225019,\n",
       " ('num-2', 'B-MISC'): 1.222114,\n",
       " ('num-2', 'I-ORG'): 1.217238,\n",
       " ('num-2', 'I-LOC'): 1.206267,\n",
       " ('num-2', 'I-MISC'): 1.188354,\n",
       " ('word.lower():(', 'O'): 0.971724,\n",
       " ('word[-3:]:(', 'O'): 0.971724,\n",
       " ('word[-2:]:(', 'O'): 0.971724,\n",
       " ('postag:Fpa', 'O'): 0.971724,\n",
       " ('postag[:2]:Fp', 'B-LOC'): 0.156431,\n",
       " ('postag[:2]:Fp', 'O'): 3.067186,\n",
       " ('-1:word.istitle()', 'B-LOC'): -1.264304,\n",
       " ('-1:word.istitle()', 'O'): 0.59816,\n",
       " ('-1:word.istitle()', 'B-ORG'): -0.527272,\n",
       " ('-1:word.istitle()', 'B-PER'): -0.663033,\n",
       " ('-1:word.istitle()', 'I-PER'): 1.004735,\n",
       " ('-1:word.istitle()', 'B-MISC'): -0.520476,\n",
       " ('-1:word.istitle()', 'I-ORG'): 0.609596,\n",
       " ('-1:word.istitle()', 'I-LOC'): -0.010236,\n",
       " ('-1:word.istitle()', 'I-MISC'): 0.312466,\n",
       " ('-1:word.isupper()', 'B-LOC'): 1.398107,\n",
       " ('-1:word.isupper()', 'O'): 2.245599,\n",
       " ('-1:word.isupper()', 'B-ORG'): -1.334807,\n",
       " ('-1:word.isupper()', 'B-PER'): -0.045442,\n",
       " ('-1:word.isupper()', 'I-PER'): -0.061403,\n",
       " ('-1:word.isupper()', 'B-MISC'): -2.258929,\n",
       " ('-1:word.isupper()', 'I-ORG'): -0.944679,\n",
       " ('-1:word.isupper()', 'I-LOC'): -0.091989,\n",
       " ('-1:word.isupper()', 'I-MISC'): -0.059269,\n",
       " ('-1:postag:NP', 'O'): 1.129478,\n",
       " ('-1:postag:NP', 'I-PER'): 0.086184,\n",
       " ('-1:postag:NP', 'I-ORG'): -0.255883,\n",
       " ('-1:postag:NP', 'I-MISC'): 0.468277,\n",
       " ('-1:postag[:2]:NP', 'O'): 1.129478,\n",
       " ('-1:postag[:2]:NP', 'I-PER'): 0.086184,\n",
       " ('-1:postag[:2]:NP', 'I-ORG'): -0.255883,\n",
       " ('-1:postag[:2]:NP', 'I-MISC'): 0.468277,\n",
       " ('+1:postag:NP', 'O'): 0.160591,\n",
       " ('+1:postag:NP', 'B-ORG'): 0.277395,\n",
       " ('+1:postag:NP', 'B-PER'): 0.099502,\n",
       " ('+1:postag[:2]:NP', 'O'): 0.160591,\n",
       " ('+1:postag[:2]:NP', 'B-ORG'): 0.277395,\n",
       " ('+1:postag[:2]:NP', 'B-PER'): 0.099502,\n",
       " ('word.lower():australia', 'B-LOC'): 0.682189,\n",
       " ('word.lower():australia', 'B-ORG'): 0.008585,\n",
       " ('word[-3:]:lia', 'B-LOC'): 0.447076,\n",
       " ('word[-3:]:lia', 'O'): -0.588226,\n",
       " ('word[-3:]:lia', 'B-ORG'): 0.018653,\n",
       " ('word[-3:]:lia', 'B-PER'): 0.002622,\n",
       " ('word[-3:]:lia', 'I-PER'): -0.7847,\n",
       " ('word[-3:]:lia', 'B-MISC'): -0.021488,\n",
       " ('word[-3:]:lia', 'I-ORG'): 0.019231,\n",
       " ('word[-3:]:lia', 'I-MISC'): 0.031031,\n",
       " ('word[-2:]:ia', 'B-LOC'): 0.568135,\n",
       " ('word[-2:]:ia', 'O'): -0.456799,\n",
       " ('word[-2:]:ia', 'B-ORG'): 0.116068,\n",
       " ('word[-2:]:ia', 'B-PER'): -1.431683,\n",
       " ('word[-2:]:ia', 'I-PER'): -0.460901,\n",
       " ('word[-2:]:ia', 'B-MISC'): -0.033878,\n",
       " ('word[-2:]:ia', 'I-ORG'): 0.016952,\n",
       " ('word[-2:]:ia', 'I-LOC'): -0.014252,\n",
       " ('word[-2:]:ia', 'I-MISC'): -9.1e-05,\n",
       " ('-1:word.lower():(', 'B-LOC'): 0.347815,\n",
       " ('-1:word.lower():(', 'O'): 0.154705,\n",
       " ('-1:word.lower():(', 'B-ORG'): 0.238942,\n",
       " ('-1:word.lower():(', 'B-PER'): -0.370596,\n",
       " ('-1:word.lower():(', 'B-MISC'): -0.018729,\n",
       " ('-1:postag:Fpa', 'B-LOC'): 0.347815,\n",
       " ('-1:postag:Fpa', 'O'): 0.154705,\n",
       " ('-1:postag:Fpa', 'B-ORG'): 0.238942,\n",
       " ('-1:postag:Fpa', 'B-PER'): -0.370596,\n",
       " ('-1:postag:Fpa', 'B-MISC'): -0.018729,\n",
       " ('-1:postag[:2]:Fp', 'B-LOC'): 0.348377,\n",
       " ('-1:postag[:2]:Fp', 'O'): 0.1314,\n",
       " ('-1:postag[:2]:Fp', 'B-ORG'): 0.208453,\n",
       " ('-1:postag[:2]:Fp', 'B-PER'): -0.081869,\n",
       " ('-1:postag[:2]:Fp', 'B-MISC'): 0.032054,\n",
       " ('+1:word.lower():)', 'B-LOC'): 0.229801,\n",
       " ('+1:word.lower():)', 'O'): 0.031092,\n",
       " ('+1:word.lower():)', 'B-ORG'): 0.002585,\n",
       " ('+1:word.lower():)', 'B-PER'): -0.333532,\n",
       " ('+1:word.lower():)', 'I-PER'): -0.305069,\n",
       " ('+1:word.lower():)', 'B-MISC'): -0.050562,\n",
       " ('+1:word.lower():)', 'I-ORG'): -0.093982,\n",
       " ('+1:word.lower():)', 'I-LOC'): 0.142115,\n",
       " ('+1:word.lower():)', 'I-MISC'): 0.130699,\n",
       " ('+1:postag:Fpt', 'B-LOC'): 0.229801,\n",
       " ('+1:postag:Fpt', 'O'): 0.031092,\n",
       " ('+1:postag:Fpt', 'B-ORG'): 0.002585,\n",
       " ('+1:postag:Fpt', 'B-PER'): -0.333532,\n",
       " ('+1:postag:Fpt', 'I-PER'): -0.305069,\n",
       " ('+1:postag:Fpt', 'B-MISC'): -0.050562,\n",
       " ('+1:postag:Fpt', 'I-ORG'): -0.093982,\n",
       " ('+1:postag:Fpt', 'I-LOC'): 0.142115,\n",
       " ('+1:postag:Fpt', 'I-MISC'): 0.130699,\n",
       " ('word.lower():)', 'O'): 0.676925,\n",
       " ('word[-3:]:)', 'O'): 0.676925,\n",
       " ('word[-2:]:)', 'O'): 0.676925,\n",
       " ('postag:Fpt', 'O'): 0.676925,\n",
       " ('+1:word.lower():,', 'B-LOC'): 0.141161,\n",
       " ('+1:word.lower():,', 'O'): -0.126607,\n",
       " ('+1:word.lower():,', 'B-ORG'): 0.01569,\n",
       " ('+1:word.lower():,', 'B-PER'): -0.03369,\n",
       " ('+1:word.lower():,', 'I-PER'): 0.153059,\n",
       " ('+1:word.lower():,', 'B-MISC'): 0.006946,\n",
       " ('+1:word.lower():,', 'I-ORG'): 0.010772,\n",
       " ('+1:word.lower():,', 'I-LOC'): 0.028288,\n",
       " ('+1:word.lower():,', 'I-MISC'): 0.00359,\n",
       " ('+1:postag:Fc', 'B-LOC'): 0.141161,\n",
       " ('+1:postag:Fc', 'O'): -0.126607,\n",
       " ('+1:postag:Fc', 'B-ORG'): 0.01569,\n",
       " ('+1:postag:Fc', 'B-PER'): -0.03369,\n",
       " ('+1:postag:Fc', 'I-PER'): 0.153059,\n",
       " ('+1:postag:Fc', 'B-MISC'): 0.006946,\n",
       " ('+1:postag:Fc', 'I-ORG'): 0.010772,\n",
       " ('+1:postag:Fc', 'I-LOC'): 0.028288,\n",
       " ('+1:postag:Fc', 'I-MISC'): 0.00359,\n",
       " ('+1:postag[:2]:Fc', 'B-LOC'): 0.141161,\n",
       " ('+1:postag[:2]:Fc', 'O'): -0.126607,\n",
       " ('+1:postag[:2]:Fc', 'B-ORG'): 0.01569,\n",
       " ('+1:postag[:2]:Fc', 'B-PER'): -0.03369,\n",
       " ('+1:postag[:2]:Fc', 'I-PER'): 0.153059,\n",
       " ('+1:postag[:2]:Fc', 'B-MISC'): 0.006946,\n",
       " ('+1:postag[:2]:Fc', 'I-ORG'): 0.010772,\n",
       " ('+1:postag[:2]:Fc', 'I-LOC'): 0.028288,\n",
       " ('+1:postag[:2]:Fc', 'I-MISC'): 0.00359,\n",
       " ('word.lower():,', 'O'): 2.013486,\n",
       " ('word[-3:]:,', 'O'): 2.013486,\n",
       " ('word[-2:]:,', 'O'): 2.013486,\n",
       " ('postag:Fc', 'O'): 2.013486,\n",
       " ('postag[:2]:Fc', 'O'): 2.013486,\n",
       " ('-1:word.lower():)', 'B-LOC'): -0.049231,\n",
       " ('-1:word.lower():)', 'O'): 0.224027,\n",
       " ('-1:word.lower():)', 'B-ORG'): -1.035976,\n",
       " ('-1:word.lower():)', 'B-PER'): 0.098615,\n",
       " ('-1:word.lower():)', 'B-MISC'): -1.054675,\n",
       " ('-1:postag:Fpt', 'B-LOC'): -0.049231,\n",
       " ('-1:postag:Fpt', 'O'): 0.224027,\n",
       " ('-1:postag:Fpt', 'B-ORG'): -1.035976,\n",
       " ('-1:postag:Fpt', 'B-PER'): 0.098615,\n",
       " ('-1:postag:Fpt', 'B-MISC'): -1.054675,\n",
       " ('+1:word.lower():25', 'B-LOC'): 0.845082,\n",
       " ('+1:word.lower():25', 'O'): 0.003702,\n",
       " ('+1:word.lower():25', 'I-LOC'): 0.95231,\n",
       " ('+1:postag:Z', 'B-LOC'): -0.127111,\n",
       " ('+1:postag:Z', 'O'): 0.145503,\n",
       " ('+1:postag:Z', 'B-ORG'): 0.024266,\n",
       " ('+1:postag:Z', 'B-PER'): -0.811179,\n",
       " ('+1:postag:Z', 'I-PER'): -1.665307,\n",
       " ('+1:postag:Z', 'B-MISC'): 0.515108,\n",
       " ('+1:postag:Z', 'I-ORG'): -0.036045,\n",
       " ('+1:postag:Z', 'I-LOC'): -0.063292,\n",
       " ('+1:postag:Z', 'I-MISC'): 0.128328,\n",
       " ('+1:postag[:2]:Z', 'B-LOC'): -0.127111,\n",
       " ('+1:postag[:2]:Z', 'O'): 0.145503,\n",
       " ('+1:postag[:2]:Z', 'B-ORG'): 0.024266,\n",
       " ('+1:postag[:2]:Z', 'B-PER'): -0.811179,\n",
       " ('+1:postag[:2]:Z', 'I-PER'): -1.665307,\n",
       " ('+1:postag[:2]:Z', 'B-MISC'): 0.515108,\n",
       " ('+1:postag[:2]:Z', 'I-ORG'): -0.036045,\n",
       " ('+1:postag[:2]:Z', 'I-LOC'): -0.063292,\n",
       " ('+1:postag[:2]:Z', 'I-MISC'): 0.128328,\n",
       " ('word.lower():25', 'O'): 0.275013,\n",
       " ('word[-3:]:25', 'O'): 0.275013,\n",
       " ('word[-2:]:25', 'O'): 0.955849,\n",
       " ('postag:Z', 'B-LOC'): -1.154634,\n",
       " ('postag:Z', 'O'): 0.572884,\n",
       " ('postag:Z', 'B-ORG'): -2.035235,\n",
       " ('postag:Z', 'B-MISC'): 0.727487,\n",
       " ('postag:Z', 'I-ORG'): -0.075692,\n",
       " ('postag:Z', 'I-LOC'): -0.391443,\n",
       " ('postag:Z', 'I-MISC'): 0.472256,\n",
       " ('postag[:2]:Z', 'B-LOC'): -1.154634,\n",
       " ('postag[:2]:Z', 'O'): 0.572884,\n",
       " ('postag[:2]:Z', 'B-ORG'): -2.035235,\n",
       " ('postag[:2]:Z', 'B-MISC'): 0.727487,\n",
       " ('postag[:2]:Z', 'I-ORG'): -0.075692,\n",
       " ('postag[:2]:Z', 'I-LOC'): -0.391443,\n",
       " ('postag[:2]:Z', 'I-MISC'): 0.472256,\n",
       " ('-1:word.lower():,', 'B-LOC'): 0.000991,\n",
       " ('-1:word.lower():,', 'O'): 0.00953,\n",
       " ('-1:word.lower():,', 'B-ORG'): 0.129747,\n",
       " ('-1:word.lower():,', 'B-PER'): 0.394145,\n",
       " ('-1:word.lower():,', 'B-MISC'): -0.035007,\n",
       " ('-1:postag:Fc', 'B-LOC'): 0.000991,\n",
       " ('-1:postag:Fc', 'O'): 0.00953,\n",
       " ('-1:postag:Fc', 'B-ORG'): 0.129747,\n",
       " ('-1:postag:Fc', 'B-PER'): 0.394145,\n",
       " ('-1:postag:Fc', 'B-MISC'): -0.035007,\n",
       " ('-1:postag[:2]:Fc', 'B-LOC'): 0.000991,\n",
       " ('-1:postag[:2]:Fc', 'O'): 0.00953,\n",
       " ('-1:postag[:2]:Fc', 'B-ORG'): 0.129747,\n",
       " ('-1:postag[:2]:Fc', 'B-PER'): 0.394145,\n",
       " ('-1:postag[:2]:Fc', 'B-MISC'): -0.035007,\n",
       " ('+1:postag:NC', 'B-LOC'): -0.359204,\n",
       " ('+1:postag:NC', 'O'): 0.50489,\n",
       " ('+1:postag:NC', 'B-ORG'): -0.265371,\n",
       " ('+1:postag:NC', 'B-PER'): 0.006749,\n",
       " ('+1:postag:NC', 'I-PER'): -0.162481,\n",
       " ('+1:postag:NC', 'B-MISC'): -0.00393,\n",
       " ('+1:postag:NC', 'I-ORG'): -0.050447,\n",
       " ('+1:postag:NC', 'I-LOC'): 0.014531,\n",
       " ('+1:postag:NC', 'I-MISC'): -0.047225,\n",
       " ('+1:postag[:2]:NC', 'B-LOC'): -0.359204,\n",
       " ('+1:postag[:2]:NC', 'O'): 0.50489,\n",
       " ('+1:postag[:2]:NC', 'B-ORG'): -0.265371,\n",
       " ('+1:postag[:2]:NC', 'B-PER'): 0.006749,\n",
       " ('+1:postag[:2]:NC', 'I-PER'): -0.162481,\n",
       " ('+1:postag[:2]:NC', 'B-MISC'): -0.00393,\n",
       " ('+1:postag[:2]:NC', 'I-ORG'): -0.050447,\n",
       " ('+1:postag[:2]:NC', 'I-LOC'): 0.014531,\n",
       " ('+1:postag[:2]:NC', 'I-MISC'): -0.047225,\n",
       " ('word.lower():may', 'O'): 2.454833,\n",
       " ('word[-2:]:ay', 'B-LOC'): 0.037923,\n",
       " ('word[-2:]:ay', 'O'): 1.878091,\n",
       " ('word[-2:]:ay', 'B-ORG'): 1.857081,\n",
       " ('word[-2:]:ay', 'B-PER'): -0.273395,\n",
       " ('word[-2:]:ay', 'I-PER'): -0.184882,\n",
       " ('word[-2:]:ay', 'I-ORG'): -0.051466,\n",
       " ('word[-2:]:ay', 'I-MISC'): -0.390676,\n",
       " ('postag:NC', 'B-LOC'): 0.12185,\n",
       " ('postag:NC', 'O'): -0.092958,\n",
       " ('postag:NC', 'B-ORG'): 0.247439,\n",
       " ('postag:NC', 'B-PER'): 0.076757,\n",
       " ('postag:NC', 'I-PER'): 0.032981,\n",
       " ('postag:NC', 'B-MISC'): 0.078337,\n",
       " ('postag:NC', 'I-ORG'): 0.024988,\n",
       " ('postag:NC', 'I-LOC'): 0.013737,\n",
       " ('postag:NC', 'I-MISC'): 0.022811,\n",
       " ('postag[:2]:NC', 'B-LOC'): 0.12185,\n",
       " ('postag[:2]:NC', 'O'): -0.092958,\n",
       " ('postag[:2]:NC', 'B-ORG'): 0.247439,\n",
       " ('postag[:2]:NC', 'B-PER'): 0.076757,\n",
       " ('postag[:2]:NC', 'I-PER'): 0.032981,\n",
       " ('postag[:2]:NC', 'B-MISC'): 0.078337,\n",
       " ('postag[:2]:NC', 'I-ORG'): 0.024988,\n",
       " ('postag[:2]:NC', 'I-LOC'): 0.013737,\n",
       " ('postag[:2]:NC', 'I-MISC'): 0.022811,\n",
       " ('-1:word.lower():25', 'O'): 0.584723,\n",
       " ('-1:postag:Z', 'B-LOC'): 0.099077,\n",
       " ('-1:postag:Z', 'O'): 0.56668,\n",
       " ('-1:postag:Z', 'B-ORG'): -0.002854,\n",
       " ('-1:postag:Z', 'B-PER'): 0.033428,\n",
       " ('-1:postag:Z', 'B-MISC'): -0.006359,\n",
       " ('-1:postag:Z', 'I-ORG'): -0.439011,\n",
       " ('-1:postag:Z', 'I-LOC'): -0.528563,\n",
       " ('-1:postag:Z', 'I-MISC'): -0.026949,\n",
       " ('-1:postag[:2]:Z', 'B-LOC'): 0.099077,\n",
       " ('-1:postag[:2]:Z', 'O'): 0.56668,\n",
       " ('-1:postag[:2]:Z', 'B-ORG'): -0.002854,\n",
       " ('-1:postag[:2]:Z', 'B-PER'): 0.033428,\n",
       " ('-1:postag[:2]:Z', 'B-MISC'): -0.006359,\n",
       " ('-1:postag[:2]:Z', 'I-ORG'): -0.439011,\n",
       " ('-1:postag[:2]:Z', 'I-LOC'): -0.528563,\n",
       " ('-1:postag[:2]:Z', 'I-MISC'): -0.026949,\n",
       " ('-1:word.lower():may', 'O'): 0.306246,\n",
       " ('-1:postag:NC', 'B-LOC'): -0.368968,\n",
       " ('-1:postag:NC', 'O'): 0.184368,\n",
       " ('-1:postag:NC', 'B-ORG'): -0.196712,\n",
       " ('-1:postag:NC', 'B-PER'): -0.167368,\n",
       " ('-1:postag:NC', 'I-PER'): 0.031352,\n",
       " ('-1:postag:NC', 'B-MISC'): 0.085559,\n",
       " ('-1:postag:NC', 'I-ORG'): -0.016964,\n",
       " ('-1:postag:NC', 'I-LOC'): 0.01771,\n",
       " ('-1:postag:NC', 'I-MISC'): 0.109966,\n",
       " ('-1:postag[:2]:NC', 'B-LOC'): -0.368968,\n",
       " ('-1:postag[:2]:NC', 'O'): 0.184368,\n",
       " ('-1:postag[:2]:NC', 'B-ORG'): -0.196712,\n",
       " ('-1:postag[:2]:NC', 'B-PER'): -0.167368,\n",
       " ('-1:postag[:2]:NC', 'I-PER'): 0.031352,\n",
       " ('-1:postag[:2]:NC', 'B-MISC'): 0.085559,\n",
       " ('-1:postag[:2]:NC', 'I-ORG'): -0.016964,\n",
       " ('-1:postag[:2]:NC', 'I-LOC'): 0.01771,\n",
       " ('-1:postag[:2]:NC', 'I-MISC'): 0.109966,\n",
       " ('+1:word.lower():efe', 'B-ORG'): 0.08626,\n",
       " ('+1:word.lower():efe', 'I-ORG'): 0.086923,\n",
       " ('word.lower():efe', 'O'): 1.004702,\n",
       " ('word.lower():efe', 'B-ORG'): 3.995218,\n",
       " ('word.lower():efe', 'I-ORG'): 0.589651,\n",
       " ('word[-3:]:EFE', 'O'): -0.549605,\n",
       " ('word[-3:]:EFE', 'B-ORG'): 1.491745,\n",
       " ('word[-3:]:EFE', 'I-ORG'): 0.377126,\n",
       " ('word[-2:]:FE', 'O'): 0.492575,\n",
       " ('word[-2:]:FE', 'B-ORG'): 0.440909,\n",
       " ('word[-2:]:FE', 'B-MISC'): -0.025228,\n",
       " ('word[-2:]:FE', 'I-ORG'): 0.387494,\n",
       " ('word[-2:]:FE', 'I-MISC'): -0.021408,\n",
       " ('-1:word.lower():efe', 'B-LOC'): 2.549316,\n",
       " ('-1:word.lower():efe', 'O'): 3.431946,\n",
       " ('-1:word.lower():efe', 'B-PER'): 3.185301,\n",
       " ('-1:word.lower():efe', 'B-MISC'): 0.04523,\n",
       " ('+1:word.lower():.', 'B-LOC'): 0.166513,\n",
       " ('+1:word.lower():.', 'O'): -0.036433,\n",
       " ('+1:word.lower():.', 'B-ORG'): 0.001021,\n",
       " ('+1:word.lower():.', 'B-PER'): 0.015449,\n",
       " ('+1:word.lower():.', 'I-PER'): 0.151529,\n",
       " ('+1:word.lower():.', 'B-MISC'): 0.018152,\n",
       " ('+1:word.lower():.', 'I-ORG'): -0.006152,\n",
       " ('+1:word.lower():.', 'I-LOC'): 0.005025,\n",
       " ('+1:word.lower():.', 'I-MISC'): -0.005663,\n",
       " ('+1:postag:Fp', 'B-LOC'): 0.166513,\n",
       " ('+1:postag:Fp', 'O'): -0.036433,\n",
       " ('+1:postag:Fp', 'B-ORG'): 0.001021,\n",
       " ('+1:postag:Fp', 'B-PER'): 0.015449,\n",
       " ('+1:postag:Fp', 'I-PER'): 0.151529,\n",
       " ('+1:postag:Fp', 'B-MISC'): 0.018152,\n",
       " ('+1:postag:Fp', 'I-ORG'): -0.006152,\n",
       " ('+1:postag:Fp', 'I-LOC'): 0.005025,\n",
       " ('+1:postag:Fp', 'I-MISC'): -0.005663,\n",
       " ('word.lower():.', 'B-LOC'): 0.459923,\n",
       " ('word.lower():.', 'O'): 1.071774,\n",
       " ('word[-3:]:.', 'B-LOC'): 0.459923,\n",
       " ('word[-3:]:.', 'O'): 1.071774,\n",
       " ('word[-2:]:.', 'B-LOC'): 0.459923,\n",
       " ('word[-2:]:.', 'O'): 1.071774,\n",
       " ('postag:Fp', 'B-LOC'): 0.459923,\n",
       " ('postag:Fp', 'O'): 1.071774,\n",
       " ('EOS', 'B-LOC'): 0.189282,\n",
       " ('EOS', 'O'): 1.288211,\n",
       " ('EOS', 'I-MISC'): 0.032929,\n",
       " ('word.lower():-', 'O'): 1.231485,\n",
       " ('word[-3:]:-', 'O'): 1.231485,\n",
       " ('word[-2:]:-', 'O'): 1.231485,\n",
       " ('postag:Fg', 'O'): 1.231485,\n",
       " ('postag[:2]:Fg', 'O'): 1.231485,\n",
       " ('word.lower():el', 'B-LOC'): -0.245731,\n",
       " ('word.lower():el', 'O'): 1.055805,\n",
       " ('word.lower():el', 'B-ORG'): -0.076966,\n",
       " ('word.lower():el', 'B-PER'): -0.265204,\n",
       " ('word.lower():el', 'I-PER'): -0.354038,\n",
       " ('word.lower():el', 'B-MISC'): -0.045695,\n",
       " ('word.lower():el', 'I-ORG'): -0.042691,\n",
       " ('word.lower():el', 'I-LOC'): -0.11595,\n",
       " ('word.lower():el', 'I-MISC'): 0.050668,\n",
       " ('word[-3:]:El', 'B-LOC'): -0.025707,\n",
       " ('word[-3:]:El', 'O'): 1.601442,\n",
       " ('word[-3:]:El', 'B-ORG'): -0.049765,\n",
       " ('word[-3:]:El', 'I-PER'): -0.240659,\n",
       " ('word[-3:]:El', 'B-MISC'): -0.048737,\n",
       " ('word[-3:]:El', 'I-ORG'): -0.214764,\n",
       " ('word[-3:]:El', 'I-MISC'): -0.302256,\n",
       " ('word[-2:]:El', 'B-LOC'): -0.018789,\n",
       " ('word[-2:]:El', 'O'): 1.598182,\n",
       " ('word[-2:]:El', 'B-ORG'): -0.059906,\n",
       " ('word[-2:]:El', 'B-PER'): -0.052807,\n",
       " ('word[-2:]:El', 'I-PER'): -0.224172,\n",
       " ('word[-2:]:El', 'B-MISC'): -0.066799,\n",
       " ('word[-2:]:El', 'I-ORG'): -0.652536,\n",
       " ('word[-2:]:El', 'I-MISC'): -0.239685,\n",
       " ('postag:DA', 'B-LOC'): -0.130071,\n",
       " ('postag:DA', 'O'): 0.486407,\n",
       " ('postag:DA', 'B-ORG'): -0.234153,\n",
       " ('postag:DA', 'B-PER'): -2.138352,\n",
       " ('postag:DA', 'I-PER'): -0.04063,\n",
       " ('postag:DA', 'B-MISC'): -0.37627,\n",
       " ('postag:DA', 'I-ORG'): 0.024396,\n",
       " ('postag:DA', 'I-LOC'): -0.012868,\n",
       " ('postag:DA', 'I-MISC'): -0.007082,\n",
       " ('postag[:2]:DA', 'B-LOC'): -0.130071,\n",
       " ('postag[:2]:DA', 'O'): 0.486407,\n",
       " ('postag[:2]:DA', 'B-ORG'): -0.234153,\n",
       " ('postag[:2]:DA', 'B-PER'): -2.138352,\n",
       " ('postag[:2]:DA', 'I-PER'): -0.04063,\n",
       " ('postag[:2]:DA', 'B-MISC'): -0.37627,\n",
       " ('postag[:2]:DA', 'I-ORG'): 0.024396,\n",
       " ('postag[:2]:DA', 'I-LOC'): -0.012868,\n",
       " ('postag[:2]:DA', 'I-MISC'): -0.007082,\n",
       " ('+1:word.lower():abogado', 'O'): 0.000542,\n",
       " ('word.lower():abogado', 'B-PER'): 2.104257,\n",
       " ('word[-3:]:ado', 'B-LOC'): -0.477137,\n",
       " ('word[-3:]:ado', 'O'): -0.119128,\n",
       " ('word[-3:]:ado', 'B-ORG'): 0.032781,\n",
       " ('word[-3:]:ado', 'B-PER'): -0.383318,\n",
       " ('word[-3:]:ado', 'I-PER'): 0.138922,\n",
       " ('word[-3:]:ado', 'B-MISC'): -0.267928,\n",
       " ('word[-3:]:ado', 'I-ORG'): 0.004336,\n",
       " ('word[-3:]:ado', 'I-LOC'): -1.157844,\n",
       " ('word[-3:]:ado', 'I-MISC'): -0.30488,\n",
       " ('word[-2:]:do', 'B-LOC'): -0.304082,\n",
       " ('word[-2:]:do', 'O'): 0.451897,\n",
       " ('word[-2:]:do', 'B-ORG'): 0.008832,\n",
       " ('word[-2:]:do', 'B-PER'): 0.262825,\n",
       " ('word[-2:]:do', 'I-PER'): -0.000916,\n",
       " ('word[-2:]:do', 'B-MISC'): -0.148708,\n",
       " ('word[-2:]:do', 'I-ORG'): -0.121789,\n",
       " ('word[-2:]:do', 'I-LOC'): -0.045773,\n",
       " ('word[-2:]:do', 'I-MISC'): -0.441727,\n",
       " ('-1:word.lower():el', 'B-LOC'): -0.023315,\n",
       " ('-1:word.lower():el', 'O'): -0.021764,\n",
       " ('-1:word.lower():el', 'B-ORG'): 0.408267,\n",
       " ('-1:word.lower():el', 'B-PER'): -0.656063,\n",
       " ('-1:word.lower():el', 'I-PER'): -0.046436,\n",
       " ('-1:word.lower():el', 'B-MISC'): -0.001229,\n",
       " ('-1:word.lower():el', 'I-ORG'): 0.357352,\n",
       " ('-1:word.lower():el', 'I-LOC'): -0.073133,\n",
       " ('-1:word.lower():el', 'I-MISC'): 0.512423,\n",
       " ('-1:postag:DA', 'B-LOC'): -0.412576,\n",
       " ('-1:postag:DA', 'O'): -0.29677,\n",
       " ('-1:postag:DA', 'B-ORG'): 0.151325,\n",
       " ('-1:postag:DA', 'B-PER'): -0.917494,\n",
       " ('-1:postag:DA', 'I-PER'): 0.627853,\n",
       " ('-1:postag:DA', 'B-MISC'): 0.013931,\n",
       " ('-1:postag:DA', 'I-ORG'): 0.144162,\n",
       " ('-1:postag:DA', 'I-LOC'): 0.054085,\n",
       " ('-1:postag:DA', 'I-MISC'): 0.103626,\n",
       " ('-1:postag[:2]:DA', 'B-LOC'): -0.412576,\n",
       " ('-1:postag[:2]:DA', 'O'): -0.29677,\n",
       " ('-1:postag[:2]:DA', 'B-ORG'): 0.151325,\n",
       " ('-1:postag[:2]:DA', 'B-PER'): -0.917494,\n",
       " ('-1:postag[:2]:DA', 'I-PER'): 0.627853,\n",
       " ('-1:postag[:2]:DA', 'B-MISC'): 0.013931,\n",
       " ('-1:postag[:2]:DA', 'I-ORG'): 0.144162,\n",
       " ('-1:postag[:2]:DA', 'I-LOC'): 0.054085,\n",
       " ('-1:postag[:2]:DA', 'I-MISC'): 0.103626,\n",
       " ('+1:word.lower():general', 'B-LOC'): 0.059785,\n",
       " ('+1:word.lower():general', 'B-ORG'): -0.767682,\n",
       " ('+1:word.lower():general', 'B-PER'): 0.680418,\n",
       " ('+1:word.lower():general', 'B-MISC'): 0.33055,\n",
       " ('+1:word.lower():general', 'I-MISC'): -0.574342,\n",
       " ('+1:postag:AQ', 'B-LOC'): -0.222912,\n",
       " ('+1:postag:AQ', 'O'): 0.712166,\n",
       " ('+1:postag:AQ', 'B-ORG'): -0.036046,\n",
       " ('+1:postag:AQ', 'B-PER'): 0.101212,\n",
       " ('+1:postag:AQ', 'I-PER'): -0.348791,\n",
       " ('+1:postag:AQ', 'B-MISC'): 0.034704,\n",
       " ('+1:postag:AQ', 'I-ORG'): -0.089739,\n",
       " ('+1:postag:AQ', 'I-LOC'): -0.055159,\n",
       " ('+1:postag:AQ', 'I-MISC'): -0.010252,\n",
       " ('+1:postag[:2]:AQ', 'B-LOC'): -0.222912,\n",
       " ('+1:postag[:2]:AQ', 'O'): 0.712166,\n",
       " ('+1:postag[:2]:AQ', 'B-ORG'): -0.036046,\n",
       " ('+1:postag[:2]:AQ', 'B-PER'): 0.101212,\n",
       " ('+1:postag[:2]:AQ', 'I-PER'): -0.348791,\n",
       " ('+1:postag[:2]:AQ', 'B-MISC'): 0.034704,\n",
       " ('+1:postag[:2]:AQ', 'I-ORG'): -0.089739,\n",
       " ('+1:postag[:2]:AQ', 'I-LOC'): -0.055159,\n",
       " ('+1:postag[:2]:AQ', 'I-MISC'): -0.010252,\n",
       " ('word.lower():general', 'O'): -0.007334,\n",
       " ('word.lower():general', 'B-ORG'): 0.050159,\n",
       " ('word.lower():general', 'I-PER'): 0.935793,\n",
       " ('word.lower():general', 'I-ORG'): -0.520671,\n",
       " ('word.lower():general', 'I-MISC'): 0.102051,\n",
       " ('word[-3:]:ral', 'B-LOC'): 1.014766,\n",
       " ('word[-3:]:ral', 'O'): -0.017765,\n",
       " ('word[-3:]:ral', 'B-ORG'): -0.005583,\n",
       " ('word[-3:]:ral', 'B-MISC'): 0.002295,\n",
       " ('word[-3:]:ral', 'I-ORG'): -0.051607,\n",
       " ('word[-3:]:ral', 'I-LOC'): 0.004709,\n",
       " ('word[-3:]:ral', 'I-MISC'): 0.172472,\n",
       " ('word[-2:]:al', 'B-LOC'): -0.479416,\n",
       " ('word[-2:]:al', 'O'): 0.564318,\n",
       " ('word[-2:]:al', 'B-ORG'): -0.04701,\n",
       " ('word[-2:]:al', 'B-PER'): -1.170656,\n",
       " ('word[-2:]:al', 'I-PER'): -0.988029,\n",
       " ('word[-2:]:al', 'B-MISC'): 0.104324,\n",
       " ('word[-2:]:al', 'I-ORG'): 0.230695,\n",
       " ('word[-2:]:al', 'I-LOC'): 0.007749,\n",
       " ('word[-2:]:al', 'I-MISC'): 0.291667,\n",
       " ('postag:AQ', 'B-LOC'): 0.029141,\n",
       " ('postag:AQ', 'O'): 0.211884,\n",
       " ('postag:AQ', 'B-ORG'): 0.162213,\n",
       " ('postag:AQ', 'B-PER'): 0.089508,\n",
       " ('postag:AQ', 'I-PER'): -0.009786,\n",
       " ('postag:AQ', 'B-MISC'): 0.24804,\n",
       " ('postag:AQ', 'I-ORG'): 0.13166,\n",
       " ('postag:AQ', 'I-LOC'): -0.030873,\n",
       " ('postag:AQ', 'I-MISC'): 0.034511,\n",
       " ('postag[:2]:AQ', 'B-LOC'): 0.029141,\n",
       " ('postag[:2]:AQ', 'O'): 0.211884,\n",
       " ('postag[:2]:AQ', 'B-ORG'): 0.162213,\n",
       " ('postag[:2]:AQ', 'B-PER'): 0.089508,\n",
       " ('postag[:2]:AQ', 'I-PER'): -0.009786,\n",
       " ('postag[:2]:AQ', 'B-MISC'): 0.24804,\n",
       " ('postag[:2]:AQ', 'I-ORG'): 0.13166,\n",
       " ('postag[:2]:AQ', 'I-LOC'): -0.030873,\n",
       " ('postag[:2]:AQ', 'I-MISC'): 0.034511,\n",
       " ('-1:word.lower():abogado', 'I-PER'): 2.245651,\n",
       " ('+1:word.lower():del', 'B-LOC'): 0.170294,\n",
       " ('+1:word.lower():del', 'O'): 0.039107,\n",
       " ('+1:word.lower():del', 'B-ORG'): -0.594668,\n",
       " ('+1:word.lower():del', 'B-PER'): -0.016237,\n",
       " ('+1:word.lower():del', 'I-PER'): -1.923252,\n",
       " ('+1:word.lower():del', 'B-MISC'): 0.024287,\n",
       " ('+1:word.lower():del', 'I-ORG'): -0.129099,\n",
       " ('+1:word.lower():del', 'I-LOC'): -0.025577,\n",
       " ('+1:word.lower():del', 'I-MISC'): 0.301241,\n",
       " ('+1:postag:SP', 'B-LOC'): -0.143502,\n",
       " ('+1:postag:SP', 'O'): 0.469517,\n",
       " ('+1:postag:SP', 'B-ORG'): -0.057195,\n",
       " ('+1:postag:SP', 'B-PER'): -0.184885,\n",
       " ('+1:postag:SP', 'I-PER'): -0.312947,\n",
       " ('+1:postag:SP', 'B-MISC'): -0.028105,\n",
       " ('+1:postag:SP', 'I-ORG'): -0.01562,\n",
       " ('+1:postag:SP', 'I-LOC'): -0.002731,\n",
       " ('+1:postag:SP', 'I-MISC'): 0.068466,\n",
       " ('+1:postag[:2]:SP', 'B-LOC'): -0.143502,\n",
       " ('+1:postag[:2]:SP', 'O'): 0.469517,\n",
       " ('+1:postag[:2]:SP', 'B-ORG'): -0.057195,\n",
       " ('+1:postag[:2]:SP', 'B-PER'): -0.184885,\n",
       " ('+1:postag[:2]:SP', 'I-PER'): -0.312947,\n",
       " ('+1:postag[:2]:SP', 'B-MISC'): -0.028105,\n",
       " ('+1:postag[:2]:SP', 'I-ORG'): -0.01562,\n",
       " ('+1:postag[:2]:SP', 'I-LOC'): -0.002731,\n",
       " ('+1:postag[:2]:SP', 'I-MISC'): 0.068466,\n",
       " ('word.lower():del', 'O'): -0.016716,\n",
       " ('word.lower():del', 'B-PER'): 0.076891,\n",
       " ('word.lower():del', 'I-PER'): 0.199426,\n",
       " ('word.lower():del', 'B-MISC'): 0.517126,\n",
       " ('word.lower():del', 'I-ORG'): 0.007868,\n",
       " ('word.lower():del', 'I-LOC'): 0.205619,\n",
       " ('word.lower():del', 'I-MISC'): -0.001576,\n",
       " ('word[-3:]:del', 'O'): -0.146664,\n",
       " ('word[-3:]:del', 'B-ORG'): -0.123271,\n",
       " ('word[-3:]:del', 'B-PER'): -1.161285,\n",
       " ('word[-3:]:del', 'I-PER'): 0.471553,\n",
       " ('word[-3:]:del', 'I-ORG'): 0.007728,\n",
       " ('word[-3:]:del', 'I-LOC'): 0.326996,\n",
       " ('word[-3:]:del', 'I-MISC'): -0.000397,\n",
       " ('word[-2:]:el', 'B-LOC'): -0.330308,\n",
       " ('word[-2:]:el', 'O'): -0.669937,\n",
       " ('word[-2:]:el', 'B-ORG'): -0.182024,\n",
       " ('word[-2:]:el', 'B-PER'): 0.05379,\n",
       " ('word[-2:]:el', 'I-PER'): 0.236645,\n",
       " ('word[-2:]:el', 'B-MISC'): -1.615128,\n",
       " ('word[-2:]:el', 'I-ORG'): -0.004193,\n",
       " ('word[-2:]:el', 'I-LOC'): 0.092476,\n",
       " ('word[-2:]:el', 'I-MISC'): 0.016798,\n",
       " ('postag:SP', 'B-LOC'): -1.529582,\n",
       " ('postag:SP', 'O'): 0.399294,\n",
       " ('postag:SP', 'B-ORG'): -1.322345,\n",
       " ('postag:SP', 'B-PER'): -1.57376,\n",
       " ('postag:SP', 'I-PER'): -0.145559,\n",
       " ('postag:SP', 'B-MISC'): -2.188203,\n",
       " ('postag:SP', 'I-ORG'): 0.000266,\n",
       " ('postag:SP', 'I-LOC'): 0.040824,\n",
       " ('postag:SP', 'I-MISC'): 0.019249,\n",
       " ('postag[:2]:SP', 'B-LOC'): -1.529582,\n",
       " ('postag[:2]:SP', 'O'): 0.399294,\n",
       " ('postag[:2]:SP', 'B-ORG'): -1.322345,\n",
       " ('postag[:2]:SP', 'B-PER'): -1.57376,\n",
       " ('postag[:2]:SP', 'I-PER'): -0.145559,\n",
       " ('postag[:2]:SP', 'B-MISC'): -2.188203,\n",
       " ('postag[:2]:SP', 'I-ORG'): 0.000266,\n",
       " ('postag[:2]:SP', 'I-LOC'): 0.040824,\n",
       " ('postag[:2]:SP', 'I-MISC'): 0.019249,\n",
       " ('-1:word.lower():general', 'O'): -0.014743,\n",
       " ('-1:word.lower():general', 'B-PER'): 0.778401,\n",
       " ('-1:word.lower():general', 'I-PER'): 0.030389,\n",
       " ('-1:word.lower():general', 'I-ORG'): 0.779367,\n",
       " ('-1:word.lower():general', 'I-MISC'): -0.001406,\n",
       " ('-1:postag:AQ', 'B-LOC'): -0.203778,\n",
       " ('-1:postag:AQ', 'O'): 0.032389,\n",
       " ('-1:postag:AQ', 'B-ORG'): -0.022122,\n",
       " ('-1:postag:AQ', 'B-PER'): -0.165296,\n",
       " ('-1:postag:AQ', 'I-PER'): 0.191403,\n",
       " ('-1:postag:AQ', 'B-MISC'): -0.096297,\n",
       " ('-1:postag:AQ', 'I-ORG'): 0.001182,\n",
       " ('-1:postag:AQ', 'I-LOC'): 0.013687,\n",
       " ('-1:postag:AQ', 'I-MISC'): 0.017209,\n",
       " ('-1:postag[:2]:AQ', 'B-LOC'): -0.203778,\n",
       " ('-1:postag[:2]:AQ', 'O'): 0.032389,\n",
       " ('-1:postag[:2]:AQ', 'B-ORG'): -0.022122,\n",
       " ('-1:postag[:2]:AQ', 'B-PER'): -0.165296,\n",
       " ('-1:postag[:2]:AQ', 'I-PER'): 0.191403,\n",
       " ('-1:postag[:2]:AQ', 'B-MISC'): -0.096297,\n",
       " ('-1:postag[:2]:AQ', 'I-ORG'): 0.001182,\n",
       " ('-1:postag[:2]:AQ', 'I-LOC'): 0.013687,\n",
       " ('-1:postag[:2]:AQ', 'I-MISC'): 0.017209,\n",
       " ('+1:word.lower():estado', 'O'): -0.077625,\n",
       " ('+1:word.lower():estado', 'I-PER'): 0.836873,\n",
       " ('+1:word.lower():estado', 'I-ORG'): 0.013164,\n",
       " ('+1:word.lower():estado', 'I-MISC'): 0.056539,\n",
       " ('word.lower():estado', 'O'): 0.251377,\n",
       " ('word.lower():estado', 'B-ORG'): 2.687111,\n",
       " ('word.lower():estado', 'I-PER'): 0.354851,\n",
       " ('word.lower():estado', 'B-MISC'): -0.396571,\n",
       " ('word.lower():estado', 'I-ORG'): 0.883201,\n",
       " ('word.lower():estado', 'I-MISC'): 0.098648,\n",
       " ('-1:word.lower():del', 'B-LOC'): -0.792298,\n",
       " ('-1:word.lower():del', 'B-ORG'): 0.917819,\n",
       " ('-1:word.lower():del', 'B-PER'): -2.924117,\n",
       " ('-1:word.lower():del', 'I-PER'): 0.928139,\n",
       " ('-1:word.lower():del', 'B-MISC'): 0.868969,\n",
       " ('-1:word.lower():del', 'I-ORG'): 0.83303,\n",
       " ('-1:word.lower():del', 'I-LOC'): 0.413913,\n",
       " ('-1:word.lower():del', 'I-MISC'): 0.018219,\n",
       " ('-1:postag:SP', 'B-LOC'): -0.003841,\n",
       " ('-1:postag:SP', 'O'): -0.179715,\n",
       " ('-1:postag:SP', 'B-ORG'): -0.013546,\n",
       " ('-1:postag:SP', 'B-PER'): -0.139134,\n",
       " ('-1:postag:SP', 'I-PER'): 0.057297,\n",
       " ('-1:postag:SP', 'B-MISC'): 0.042653,\n",
       " ('-1:postag:SP', 'I-ORG'): 0.271124,\n",
       " ('-1:postag:SP', 'I-LOC'): 0.129224,\n",
       " ('-1:postag:SP', 'I-MISC'): 0.184434,\n",
       " ('-1:postag[:2]:SP', 'B-LOC'): -0.003841,\n",
       " ('-1:postag[:2]:SP', 'O'): -0.179715,\n",
       " ('-1:postag[:2]:SP', 'B-ORG'): -0.013546,\n",
       " ('-1:postag[:2]:SP', 'B-PER'): -0.139134,\n",
       " ('-1:postag[:2]:SP', 'I-PER'): 0.057297,\n",
       " ('-1:postag[:2]:SP', 'B-MISC'): 0.042653,\n",
       " ('-1:postag[:2]:SP', 'I-ORG'): 0.271124,\n",
       " ('-1:postag[:2]:SP', 'I-LOC'): 0.129224,\n",
       " ('-1:postag[:2]:SP', 'I-MISC'): 0.184434,\n",
       " ('-1:word.lower():estado', 'O'): 0.899389,\n",
       " ('-1:word.lower():estado', 'I-PER'): 0.00147,\n",
       " ('-1:word.lower():estado', 'I-ORG'): -0.680202,\n",
       " ('-1:word.lower():estado', 'I-MISC'): -0.00076,\n",
       " ('+1:postag:VMI', 'B-LOC'): 0.084115,\n",
       " ('+1:postag:VMI', 'O'): -0.37102,\n",
       " ('+1:postag:VMI', 'B-ORG'): 0.124522,\n",
       " ('+1:postag:VMI', 'B-PER'): 0.731865,\n",
       " ('+1:postag:VMI', 'I-PER'): -0.00147,\n",
       " ('+1:postag:VMI', 'B-MISC'): -0.183329,\n",
       " ('+1:postag:VMI', 'I-ORG'): 0.023541,\n",
       " ('+1:postag:VMI', 'I-LOC'): -0.179084,\n",
       " ('+1:postag:VMI', 'I-MISC'): 0.309301,\n",
       " ('+1:postag[:2]:VM', 'B-LOC'): -0.628434,\n",
       " ('+1:postag[:2]:VM', 'O'): 0.170625,\n",
       " ('+1:postag[:2]:VM', 'B-ORG'): 0.139258,\n",
       " ('+1:postag[:2]:VM', 'B-PER'): 0.150742,\n",
       " ('+1:postag[:2]:VM', 'I-PER'): -0.140337,\n",
       " ('+1:postag[:2]:VM', 'B-MISC'): -0.229522,\n",
       " ('+1:postag[:2]:VM', 'I-ORG'): -0.083331,\n",
       " ('+1:postag[:2]:VM', 'I-LOC'): -0.261592,\n",
       " ('+1:postag[:2]:VM', 'I-MISC'): -0.753966,\n",
       " ('word[-3:]:ryl', 'B-PER'): 0.000763,\n",
       " ('word[-2:]:yl', 'B-ORG'): 0.279847,\n",
       " ('word[-2:]:yl', 'B-PER'): -1e-06,\n",
       " ('postag:VMI', 'B-LOC'): -0.1254,\n",
       " ('postag:VMI', 'O'): 0.174619,\n",
       " ('postag:VMI', 'B-ORG'): 0.569603,\n",
       " ('postag:VMI', 'B-PER'): 0.250677,\n",
       " ('postag:VMI', 'I-PER'): -0.747529,\n",
       " ('postag:VMI', 'B-MISC'): -0.110674,\n",
       " ('postag:VMI', 'I-ORG'): -0.743141,\n",
       " ('postag:VMI', 'I-LOC'): -0.162317,\n",
       " ('postag:VMI', 'I-MISC'): -0.808851,\n",
       " ('postag[:2]:VM', 'B-LOC'): -0.074192,\n",
       " ('postag[:2]:VM', 'O'): 0.43473,\n",
       " ('postag[:2]:VM', 'B-ORG'): 0.038106,\n",
       " ('postag[:2]:VM', 'B-PER'): 0.123712,\n",
       " ('postag[:2]:VM', 'I-PER'): -0.64591,\n",
       " ('postag[:2]:VM', 'B-MISC'): -0.058578,\n",
       " ('postag[:2]:VM', 'I-ORG'): -0.658022,\n",
       " ('postag[:2]:VM', 'I-LOC'): -0.126061,\n",
       " ('postag[:2]:VM', 'I-MISC'): -0.004707,\n",
       " ('+1:word.lower():williams', 'B-PER'): 0.5479,\n",
       " ('word.lower():williams', 'B-ORG'): 1.340459,\n",
       " ('word.lower():williams', 'I-PER'): 0.547979,\n",
       " ('word[-3:]:ams', 'B-ORG'): 1.322286,\n",
       " ('word[-3:]:ams', 'I-PER'): 0.866544,\n",
       " ('word[-2:]:ms', 'B-ORG'): 1.276048,\n",
       " ('word[-2:]:ms', 'I-PER'): 0.487567,\n",
       " ('-1:postag:VMI', 'B-LOC'): -0.451636,\n",
       " ('-1:postag:VMI', 'O'): -0.188519,\n",
       " ('-1:postag:VMI', 'B-ORG'): -0.180405,\n",
       " ('-1:postag:VMI', 'B-PER'): 0.355596,\n",
       " ('-1:postag:VMI', 'I-PER'): 0.147367,\n",
       " ('-1:postag:VMI', 'B-MISC'): -0.663812,\n",
       " ('-1:postag:VMI', 'I-ORG'): -0.424297,\n",
       " ('-1:postag:VMI', 'I-LOC'): -0.342772,\n",
       " ('-1:postag:VMI', 'I-MISC'): -0.024287,\n",
       " ('-1:postag[:2]:VM', 'B-LOC'): -0.652958,\n",
       " ('-1:postag[:2]:VM', 'O'): -0.009612,\n",
       " ('-1:postag[:2]:VM', 'B-ORG'): -0.369514,\n",
       " ('-1:postag[:2]:VM', 'B-PER'): 0.270452,\n",
       " ('-1:postag[:2]:VM', 'I-PER'): 0.067849,\n",
       " ('-1:postag[:2]:VM', 'B-MISC'): -0.008474,\n",
       " ('-1:postag[:2]:VM', 'I-ORG'): -0.188443,\n",
       " ('-1:postag[:2]:VM', 'I-LOC'): -0.369035,\n",
       " ('-1:postag[:2]:VM', 'I-MISC'): 0.026824,\n",
       " ('-1:word.lower():williams', 'I-ORG'): 0.196887,\n",
       " ('+1:word.lower():subray', 'B-PER'): 1.303111,\n",
       " ('word.lower():subray', 'O'): 0.158703,\n",
       " ('word[-3:]:ay', 'O'): 0.509426,\n",
       " ('word[-2:]:y', 'O'): 0.831154,\n",
       " ('+1:word.lower():hoy', 'B-LOC'): -0.029394,\n",
       " ('+1:word.lower():hoy', 'O'): 0.065381,\n",
       " ('+1:word.lower():hoy', 'B-ORG'): 0.647631,\n",
       " ('+1:postag:RG', 'B-LOC'): -0.11222,\n",
       " ('+1:postag:RG', 'O'): 0.422532,\n",
       " ('+1:postag:RG', 'B-ORG'): 0.007633,\n",
       " ('+1:postag:RG', 'B-PER'): 0.035807,\n",
       " ('+1:postag:RG', 'I-PER'): -0.499132,\n",
       " ('+1:postag:RG', 'B-MISC'): -0.02836,\n",
       " ('+1:postag:RG', 'I-ORG'): -0.089342,\n",
       " ('+1:postag:RG', 'I-LOC'): -0.419339,\n",
       " ('+1:postag:RG', 'I-MISC'): 0.020422,\n",
       " ('+1:postag[:2]:RG', 'B-LOC'): -0.11222,\n",
       " ('+1:postag[:2]:RG', 'O'): 0.422532,\n",
       " ('+1:postag[:2]:RG', 'B-ORG'): 0.007633,\n",
       " ('+1:postag[:2]:RG', 'B-PER'): 0.035807,\n",
       " ('+1:postag[:2]:RG', 'I-PER'): -0.499132,\n",
       " ('+1:postag[:2]:RG', 'B-MISC'): -0.02836,\n",
       " ('+1:postag[:2]:RG', 'I-ORG'): -0.089342,\n",
       " ('+1:postag[:2]:RG', 'I-LOC'): -0.419339,\n",
       " ('+1:postag[:2]:RG', 'I-MISC'): 0.020422,\n",
       " ('word.lower():hoy', 'O'): -0.053426,\n",
       " ('word.lower():hoy', 'I-ORG'): 1.588001,\n",
       " ('word[-3:]:hoy', 'O'): 0.272079,\n",
       " ('word[-2:]:oy', 'O'): 1.021769,\n",
       " ('word[-2:]:oy', 'B-PER'): 0.544902,\n",
       " ('word[-2:]:oy', 'I-ORG'): 0.017255,\n",
       " ('postag:RG', 'O'): 2.26203,\n",
       " ('postag:RG', 'B-ORG'): -0.285923,\n",
       " ('postag:RG', 'B-PER'): -0.442564,\n",
       " ('postag:RG', 'I-PER'): -0.821161,\n",
       " ('postag:RG', 'B-MISC'): -0.171676,\n",
       " ('postag:RG', 'I-ORG'): -0.036256,\n",
       " ('postag:RG', 'I-MISC'): 0.141806,\n",
       " ('postag[:2]:RG', 'O'): 2.26203,\n",
       " ('postag[:2]:RG', 'B-ORG'): -0.285923,\n",
       " ('postag[:2]:RG', 'B-PER'): -0.442564,\n",
       " ('postag[:2]:RG', 'I-PER'): -0.821161,\n",
       " ('postag[:2]:RG', 'B-MISC'): -0.171676,\n",
       " ('postag[:2]:RG', 'I-ORG'): -0.036256,\n",
       " ('postag[:2]:RG', 'I-MISC'): 0.141806,\n",
       " ('-1:word.lower():subray', 'O'): -1.024364,\n",
       " ('-1:word.lower():subray', 'B-PER'): 0.957806,\n",
       " ('+1:word.lower():la', 'B-LOC'): -0.735319,\n",
       " ('+1:word.lower():la', 'O'): 0.556482,\n",
       " ('+1:word.lower():la', 'B-ORG'): -0.10383,\n",
       " ('+1:word.lower():la', 'B-PER'): 0.357958,\n",
       " ('+1:word.lower():la', 'I-PER'): -0.036377,\n",
       " ('+1:word.lower():la', 'B-MISC'): -0.019796,\n",
       " ('+1:word.lower():la', 'I-ORG'): 0.021468,\n",
       " ('+1:word.lower():la', 'I-LOC'): 0.006077,\n",
       " ('+1:word.lower():la', 'I-MISC'): 0.002618,\n",
       " ('+1:postag:DA', 'B-LOC'): -0.079434,\n",
       " ('+1:postag:DA', 'O'): 0.248684,\n",
       " ('+1:postag:DA', 'B-ORG'): -0.032273,\n",
       " ('+1:postag:DA', 'B-PER'): -0.181341,\n",
       " ('+1:postag:DA', 'I-PER'): -0.201837,\n",
       " ('+1:postag:DA', 'B-MISC'): -0.073121,\n",
       " ('+1:postag:DA', 'I-ORG'): 0.043092,\n",
       " ('+1:postag:DA', 'I-LOC'): 0.015655,\n",
       " ('+1:postag:DA', 'I-MISC'): -0.015568,\n",
       " ('+1:postag[:2]:DA', 'B-LOC'): -0.079434,\n",
       " ('+1:postag[:2]:DA', 'O'): 0.248684,\n",
       " ('+1:postag[:2]:DA', 'B-ORG'): -0.032273,\n",
       " ('+1:postag[:2]:DA', 'B-PER'): -0.181341,\n",
       " ('+1:postag[:2]:DA', 'I-PER'): -0.201837,\n",
       " ('+1:postag[:2]:DA', 'B-MISC'): -0.073121,\n",
       " ('+1:postag[:2]:DA', 'I-ORG'): 0.043092,\n",
       " ('+1:postag[:2]:DA', 'I-LOC'): 0.015655,\n",
       " ('+1:postag[:2]:DA', 'I-MISC'): -0.015568,\n",
       " ('word.lower():la', 'B-LOC'): -0.021516,\n",
       " ('word.lower():la', 'O'): -0.009351,\n",
       " ('word.lower():la', 'B-ORG'): -0.072392,\n",
       " ('word.lower():la', 'B-PER'): -0.77044,\n",
       " ('word.lower():la', 'I-PER'): 0.144178,\n",
       " ('word.lower():la', 'B-MISC'): -0.273505,\n",
       " ('word.lower():la', 'I-ORG'): 0.007682,\n",
       " ('word.lower():la', 'I-LOC'): 0.0748,\n",
       " ('word.lower():la', 'I-MISC'): -0.00221,\n",
       " ('word[-3:]:la', 'B-LOC'): -2.189805,\n",
       " ('word[-3:]:la', 'O'): -0.263025,\n",
       " ('word[-3:]:la', 'B-ORG'): -0.279451,\n",
       " ('word[-3:]:la', 'I-PER'): 0.737928,\n",
       " ('word[-3:]:la', 'I-ORG'): 0.198816,\n",
       " ('word[-3:]:la', 'I-LOC'): 0.168953,\n",
       " ('word[-3:]:la', 'I-MISC'): 0.060985,\n",
       " ('word[-2:]:la', 'B-LOC'): -0.007209,\n",
       " ('word[-2:]:la', 'O'): -0.201558,\n",
       " ('word[-2:]:la', 'B-ORG'): -0.505998,\n",
       " ('word[-2:]:la', 'B-PER'): -0.754601,\n",
       " ('word[-2:]:la', 'I-PER'): 0.235456,\n",
       " ('word[-2:]:la', 'B-MISC'): -1.277493,\n",
       " ('word[-2:]:la', 'I-ORG'): 0.106813,\n",
       " ('word[-2:]:la', 'I-LOC'): 0.709913,\n",
       " ('word[-2:]:la', 'I-MISC'): 0.003056,\n",
       " ('-1:word.lower():hoy', 'B-LOC'): -0.076195,\n",
       " ('-1:word.lower():hoy', 'O'): -0.051775,\n",
       " ('-1:word.lower():hoy', 'B-ORG'): -0.023126,\n",
       " ('-1:word.lower():hoy', 'B-PER'): 0.093676,\n",
       " ('-1:word.lower():hoy', 'I-ORG'): 0.578356,\n",
       " ('-1:postag:RG', 'B-LOC'): -0.340056,\n",
       " ('-1:postag:RG', 'O'): -0.15332,\n",
       " ('-1:postag:RG', 'B-ORG'): -0.044007,\n",
       " ('-1:postag:RG', 'B-PER'): -0.096267,\n",
       " ('-1:postag:RG', 'I-PER'): -0.560855,\n",
       " ('-1:postag:RG', 'B-MISC'): -0.539717,\n",
       " ('-1:postag:RG', 'I-ORG'): 0.486589,\n",
       " ('-1:postag:RG', 'I-MISC'): 1.231062,\n",
       " ('-1:postag[:2]:RG', 'B-LOC'): -0.340056,\n",
       " ('-1:postag[:2]:RG', 'O'): -0.15332,\n",
       " ('-1:postag[:2]:RG', 'B-ORG'): -0.044007,\n",
       " ('-1:postag[:2]:RG', 'B-PER'): -0.096267,\n",
       " ('-1:postag[:2]:RG', 'I-PER'): -0.560855,\n",
       " ('-1:postag[:2]:RG', 'B-MISC'): -0.539717,\n",
       " ('-1:postag[:2]:RG', 'I-ORG'): 0.486589,\n",
       " ('-1:postag[:2]:RG', 'I-MISC'): 1.231062,\n",
       " ('+1:word.lower():necesidad', 'O'): 0.005678,\n",
       " ('word[-3:]:dad', 'B-LOC'): -0.019259,\n",
       " ('word[-3:]:dad', 'O'): 0.28739,\n",
       " ('word[-3:]:dad', 'B-ORG'): -0.15912,\n",
       " ('word[-3:]:dad', 'B-PER'): -1.74533,\n",
       " ('word[-3:]:dad', 'I-PER'): -0.036963,\n",
       " ('word[-3:]:dad', 'B-MISC'): 0.143188,\n",
       " ('word[-3:]:dad', 'I-ORG'): -0.000345,\n",
       " ('word[-3:]:dad', 'I-LOC'): -0.460537,\n",
       " ('word[-3:]:dad', 'I-MISC'): 0.02474,\n",
       " ('word[-2:]:ad', 'B-LOC'): -0.013395,\n",
       " ('word[-2:]:ad', 'O'): 0.352339,\n",
       " ('word[-2:]:ad', 'B-ORG'): -0.141671,\n",
       " ('word[-2:]:ad', 'B-PER'): -0.603605,\n",
       " ('word[-2:]:ad', 'I-PER'): -0.046633,\n",
       " ('word[-2:]:ad', 'B-MISC'): 0.077576,\n",
       " ('word[-2:]:ad', 'I-ORG'): -0.000577,\n",
       " ('word[-2:]:ad', 'I-LOC'): -0.686779,\n",
       " ('word[-2:]:ad', 'I-MISC'): 0.004343,\n",
       " ('-1:word.lower():la', 'B-LOC'): -0.323899,\n",
       " ('-1:word.lower():la', 'O'): -0.089103,\n",
       " ('-1:word.lower():la', 'B-ORG'): -0.002595,\n",
       " ('-1:word.lower():la', 'B-PER'): -0.789111,\n",
       " ('-1:word.lower():la', 'I-PER'): 0.127184,\n",
       " ('-1:word.lower():la', 'B-MISC'): 0.102853,\n",
       " ('-1:word.lower():la', 'I-ORG'): 0.061488,\n",
       " ('-1:word.lower():la', 'I-LOC'): 0.271213,\n",
       " ('-1:word.lower():la', 'I-MISC'): 0.008841,\n",
       " ('+1:word.lower():de', 'B-LOC'): -0.003604,\n",
       " ('+1:word.lower():de', 'O'): 0.119507,\n",
       " ('+1:word.lower():de', 'B-ORG'): -0.458083,\n",
       " ('+1:word.lower():de', 'B-PER'): -0.402041,\n",
       " ('+1:word.lower():de', 'I-PER'): -0.015102,\n",
       " ('+1:word.lower():de', 'B-MISC'): 0.096958,\n",
       " ('+1:word.lower():de', 'I-ORG'): -0.016583,\n",
       " ('+1:word.lower():de', 'I-LOC'): 0.305974,\n",
       " ('+1:word.lower():de', 'I-MISC'): 0.062631,\n",
       " ('word.lower():de', 'O'): -0.104388,\n",
       " ('word.lower():de', 'B-ORG'): -0.425936,\n",
       " ('word.lower():de', 'B-PER'): -0.1371,\n",
       " ('word.lower():de', 'I-PER'): 0.058834,\n",
       " ('word.lower():de', 'I-ORG'): 0.099405,\n",
       " ('word.lower():de', 'I-LOC'): 0.42842,\n",
       " ('word.lower():de', 'I-MISC'): 0.136649,\n",
       " ('word[-3:]:de', 'O'): -0.259998,\n",
       " ('word[-3:]:de', 'B-PER'): -0.309765,\n",
       " ('word[-3:]:de', 'I-PER'): 0.120222,\n",
       " ('word[-3:]:de', 'I-ORG'): 0.145502,\n",
       " ('word[-3:]:de', 'I-LOC'): 0.415922,\n",
       " ('word[-3:]:de', 'I-MISC'): 0.043401,\n",
       " ('word[-2:]:de', 'B-LOC'): -0.62835,\n",
       " ('word[-2:]:de', 'O'): -0.151389,\n",
       " ('word[-2:]:de', 'B-ORG'): -1.073569,\n",
       " ('word[-2:]:de', 'B-PER'): -0.178664,\n",
       " ('word[-2:]:de', 'I-PER'): 0.123041,\n",
       " ('word[-2:]:de', 'I-ORG'): 0.130176,\n",
       " ('word[-2:]:de', 'I-LOC'): 0.085386,\n",
       " ('word[-2:]:de', 'I-MISC'): 0.049276,\n",
       " ('+1:postag:VMN', 'B-LOC'): -0.315987,\n",
       " ('+1:postag:VMN', 'O'): 1.3081,\n",
       " ('+1:postag:VMN', 'B-ORG'): -0.062051,\n",
       " ('+1:postag:VMN', 'B-PER'): -0.055201,\n",
       " ('+1:postag:VMN', 'I-PER'): -0.179906,\n",
       " ('+1:postag:VMN', 'B-MISC'): -0.033401,\n",
       " ('+1:postag:VMN', 'I-ORG'): -0.018498,\n",
       " ('+1:postag:VMN', 'I-LOC'): -0.076219,\n",
       " ('+1:postag:VMN', 'I-MISC'): -0.006386,\n",
       " ('word[-3:]:mar', 'O'): -0.154735,\n",
       " ('word[-3:]:mar', 'B-ORG'): 0.736264,\n",
       " ('word[-3:]:mar', 'B-PER'): 0.000982,\n",
       " ('word[-3:]:mar', 'I-ORG'): 0.000518,\n",
       " ('word[-3:]:mar', 'I-MISC'): 0.382124,\n",
       " ('word[-2:]:ar', 'B-LOC'): -0.014806,\n",
       " ('word[-2:]:ar', 'O'): 0.863264,\n",
       " ('word[-2:]:ar', 'B-ORG'): -0.603709,\n",
       " ('word[-2:]:ar', 'B-PER'): 0.223641,\n",
       " ('word[-2:]:ar', 'I-PER'): 0.001384,\n",
       " ('word[-2:]:ar', 'B-MISC'): -0.934939,\n",
       " ('word[-2:]:ar', 'I-ORG'): -0.548963,\n",
       " ('word[-2:]:ar', 'I-LOC'): 0.002043,\n",
       " ('word[-2:]:ar', 'I-MISC'): -0.138032,\n",
       " ('postag:VMN', 'B-LOC'): 0.013961,\n",
       " ('postag:VMN', 'O'): 0.043453,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "print(labels)\n",
    "\n",
    "# dir(crf)\n",
    "crf.state_features_   # Dict with state feature coefficients  {(attr_name, label) -- coef}\n",
    "# crf.transition_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> flat f1 score: 0.7965347522966183\n",
      "> y_pred: [['B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O'], ['O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O'], ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O'], ['B-PER', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O'], ['O'], ['O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "score = metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels)\n",
    "\n",
    "print('> flat f1 score: {}'.format(score))\n",
    "print(\"> y_pred: {}\".format(y_pred[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def standardize(X, scaler=None): \n",
    "    \"\"\"\n",
    "    X: numpy array, or \n",
    "       a list of (list of dictionaries) for sequence model (e.g. CRF)\n",
    "       \n",
    "\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = {}  # vdict\n",
    "    if isinstance(X[0][0], dict):  # X[0] is a list (of dictionaries)\n",
    "        N = len(X)\n",
    "        fset = set()\n",
    "        for d in X[0]: \n",
    "            fset.update( d.keys() )\n",
    "\n",
    "        # ... fit\n",
    "        scaler = {f: {'min': np.inf , 'max': -np.inf} for f in fset}  # value dictionary\n",
    "        for f in fset: \n",
    "            \n",
    "            for j in range(N): \n",
    "                dseq = X[j]\n",
    "                fv = [di[f] for di in dseq if f in di]  # this variable across all feature dict \n",
    "                min_j, max_j = min(fv), max(fv)\n",
    "\n",
    "                if min_j < scaler[f]['min']: scaler[f]['min'] = min_j\n",
    "                if max_j > scaler[f]['max']: scaler[f]['max'] = max_j\n",
    "        # each feature now has its min, and max\n",
    "\n",
    "        # ... transform\n",
    "        for f in fset: \n",
    "            for j in range(N): \n",
    "                dseq = X[j]\n",
    "                for i, di in enumerate(dseq): \n",
    "                    if f in di: \n",
    "                        di[f] = (di[f]-scaler[f]['min'])/(scaler[f]['max']-scaler[f]['min']+0.0)\n",
    "                        # dseq[i] = di\n",
    "    else: \n",
    "        if scaler is None: scaler = MinMaxScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "    return X, scaler\n",
    "\n",
    "def transform(X, scaler=None): \n",
    "    if scaler is None or not scaler: \n",
    "        # no-op\n",
    "        return X\n",
    "\n",
    "    if isinstance(X[0][0], dict):  # X[0] is a list (of dictionaries)\n",
    "        assert isinstance(scaler, dict)\n",
    "\n",
    "        fset = list(scaler.keys())\n",
    "        N = len(X)\n",
    "\n",
    "        # transform\n",
    "        for f in fset: \n",
    "            for j in range(N): \n",
    "                dseq = X[j]\n",
    "                for i, di in enumerate(dseq): \n",
    "                    if f in di: \n",
    "                        di[f] = (di[f]-scaler[f]['min'])/(scaler[f]['max']-scaler[f]['min']+0.0)\n",
    "                        # dseq[i] = di\n",
    "    else: \n",
    "        X = scaler.transform(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   preg  plas  pres  skin  insu  mass   pedi  age            class\n",
      "0     6   148    72    35     0  33.6  0.627   50  tested_positive\n",
      "1     1    85    66    29     0  26.6  0.351   31  tested_negative\n",
      "2     8   183    64     0     0  23.3  0.672   32  tested_positive\n",
      "3     1    89    66    23    94  28.1  0.167   21  tested_negative\n",
      "4     0   137    40    35   168  43.1  2.288   33  tested_positive\n",
      "> unique values of class: ['tested_positive' 'tested_negative']\n",
      "> values(y): [0 1]\n",
      "> dim(X): (768, 8), dim(y): (768,)\n",
      "[0.71097486 0.05553948 0.82575251 0.0450562  0.95208769 0.15461324\n",
      " 0.07054015 0.63181601 0.63291146 0.04538278 0.20439439 0.89175845\n",
      " 0.84839091 0.57971155 0.6322854  0.45062654 0.34578912 0.20218305\n",
      " 0.3175605  0.24564533 0.40207693 0.30226711 0.9282186  0.28544346\n",
      " 0.63233778 0.40399456 0.69516431 0.06031307 0.5027941  0.27104115\n",
      " 0.40428383 0.61432906 0.05851874 0.04228352 0.43524466 0.20691687\n",
      " 0.66001291 0.40302963 0.17918789 0.63728229 0.72050958 0.70386856\n",
      " 0.11186769 0.90805994 0.6207162  0.97195129 0.47883635 0.05268838\n",
      " 0.36033171 0.06413342 0.0496281  0.11061044 0.07764904 0.78784436\n",
      " 0.70470201 0.02898333 0.84538907 0.37613268 0.89501867 0.16320589\n",
      " 0.01791766 0.50101769 0.03269294 0.36443604 0.34035916 0.11722561\n",
      " 0.2288824  0.48387572 0.04112754 0.2862245  0.24527549 0.37429192\n",
      " 0.80002544 0.23095594 0.05881492 0.00257094 0.08206072 0.2185131\n",
      " 0.66326323 0.11340815 0.1148674  0.00979057 0.16776375 0.05709152\n",
      " 0.60491704 0.22374514 0.45756288 0.1834406  0.72613174 0.0793139\n",
      " 0.02605726 0.26402475 0.27437888 0.28634484 0.31927211 0.47203411\n",
      " 0.08156332 0.02321794 0.14870733 0.38159907 0.87576747 0.29576135\n",
      " 0.07339034 0.03715396 0.29313225 0.3093316  0.02221099 0.409563\n",
      " 0.11250339 0.09062341 0.56582218 0.68561949 0.05504329 0.10687006\n",
      " 0.74189509 0.54025469 0.33752945 0.19918502 0.13911475 0.06052925\n",
      " 0.87096729 0.27471349 0.15713094 0.31366173 0.14422583 0.49581313\n",
      " 0.43624899 0.19337089 0.18728701 0.17790345 0.62314925 0.75868137\n",
      " 0.64631039 0.26354533 0.07798484 0.21971746 0.1204315  0.08487941\n",
      " 0.31041067 0.15610231 0.16966919 0.2866597  0.18209295 0.37225619\n",
      " 0.44393129 0.01288055 0.05590745 0.42117577 0.57128102 0.04728536\n",
      " 0.3361082  0.16190322 0.87117168 0.52127265 0.94141897 0.83603854\n",
      " 0.11958523 0.17538688 0.05419974 0.96603814 0.40008905 0.25626462\n",
      " 0.18604324 0.11780555 0.32608391 0.26512699 0.43861514 0.37113011\n",
      " 0.26008293 0.13998309 0.15022408 0.5298416  0.26982954 0.20780936\n",
      " 0.05734612 0.87287902 0.13354751 0.68049409 0.69097831 0.69903088\n",
      " 0.04871743 0.31235726 0.00270887 0.05793136 0.34066137 0.94699459\n",
      " 0.81722518 0.4717105  0.27074044 0.35127652 0.08802659 0.45364959\n",
      " 0.6861486  0.96714196 0.09877536 0.64326004 0.07428728 0.14873151\n",
      " 0.39651392 0.34396762 0.22984402 0.4031515  0.18065155 0.0463673\n",
      " 0.28761942 0.14889503 0.93652434 0.62435526 0.09797646 0.84952033\n",
      " 0.05630074 0.51186664 0.77376585 0.50809557 0.28357632 0.88755016\n",
      " 0.3109545  0.34328776 0.26227616 0.35820485 0.76358757 0.69354775\n",
      " 0.43281486 0.64030045 0.09832454 0.05732536 0.11086323 0.7939958\n",
      " 0.97969085 0.24100937 0.70239317 0.55877029 0.04201135 0.36808191\n",
      " 0.04858706 0.85863948 0.86601622 0.8345444  0.82213696 0.05942312\n",
      " 0.06164999 0.12975039 0.33762704 0.58467974 0.44731036 0.94518093\n",
      " 0.44945484 0.63697959 0.36968325 0.0883294  0.38955946 0.18516441\n",
      " 0.03994496 0.07874947 0.37270406 0.23872954 0.26023302 0.11642014\n",
      " 0.70664516 0.92450406 0.74634513 0.74141512 0.18524749 0.4234484\n",
      " 0.30319623 0.35452802 0.78770725 0.68324488 0.06013949 0.54532317\n",
      " 0.75370614 0.07689672 0.14024146 0.04541101 0.47880707 0.2904845\n",
      " 0.19164245 0.09432489 0.31461925 0.16159848 0.47438845 0.52665765\n",
      " 0.34980165 0.59113638 0.11996324 0.47045297 0.59469932 0.45929679\n",
      " 0.07375554 0.23895206 0.06106    0.27681018 0.70091831 0.49463697\n",
      " 0.42696561 0.7342284  0.24734251 0.16696036 0.45306113 0.39652184\n",
      " 0.86915746 0.40922797 0.07926654 0.50708551 0.25057926 0.26439352\n",
      " 0.65276729 0.12666758 0.4929928  0.3852959  0.08309823 0.22496743\n",
      " 0.37646241 0.27683804 0.60257539 0.18737009 0.04517247 0.7019933\n",
      " 0.23060858 0.73293128 0.29639356 0.15465797 0.15303333 0.75410783\n",
      " 0.16444076 0.23720486 0.34765174 0.86562052 0.17893583 0.15972663\n",
      " 0.62268562 0.08045739 0.93193436 0.1972851  0.05220998 0.67391532\n",
      " 0.64607144 0.27164201 0.79316582 0.8672439  0.18839946 0.09643022\n",
      " 0.00475025 0.30845027 0.38086074 0.50214358 0.41925018 0.24174359\n",
      " 0.06450895 0.01397119 0.20029904 0.32154191 0.04608525 0.08787413\n",
      " 0.24500091 0.71583069 0.46728274 0.92034175 0.31951139 0.8727232\n",
      " 0.80441635 0.60649572 0.24733635 0.72220405 0.4524895  0.25817799\n",
      " 0.28837421 0.04552372 0.04049651 0.19044376 0.94866386 0.10791332\n",
      " 0.10489679 0.15971226 0.47035493 0.76002776 0.04319122 0.13791122\n",
      " 0.79230129 0.2738745  0.2040981  0.04474049 0.18408577 0.17608072\n",
      " 0.10927028 0.10218613 0.37356836 0.36406146 0.46853433 0.26667783\n",
      " 0.12860259 0.84182281 0.11652584 0.14940567 0.72444032 0.56708765\n",
      " 0.18809408 0.24324464 0.04108254 0.8014429  0.12636699 0.34954647\n",
      " 0.39353261 0.10519825 0.6834135  0.4979777  0.23905332 0.05928232\n",
      " 0.94748925 0.73217869 0.28985684 0.19196677 0.6554541  0.20911875\n",
      " 0.37208825 0.49786201 0.19207787 0.63143973 0.03941889 0.18725453\n",
      " 0.35154095 0.08541017 0.21695427 0.206201   0.78821863 0.73081024\n",
      " 0.01925828 0.69260576 0.2827023  0.0878947  0.11465406 0.11724319\n",
      " 0.06043511 0.20583748 0.15946993 0.70888488 0.69291902 0.49279368\n",
      " 0.02910734 0.38695797 0.69579908 0.09206609 0.22147185 0.47971043\n",
      " 0.25765763 0.99559068 0.10210096 0.09431011 0.15472387 0.14253537\n",
      " 0.03198427 0.32715807 0.11583521 0.48658652 0.23700563 0.89873371\n",
      " 0.46206755 0.09799097 0.85058279 0.54018626 0.35441306 0.02821632\n",
      " 0.22111764 0.08101867 0.40067522 0.12597592 0.03705138 0.17196962\n",
      " 0.55581358 0.81618013 0.59327818 0.24805728 0.2525909  0.38406836\n",
      " 0.16052622 0.215875   0.18849736 0.1868433  0.25081333 0.34048096\n",
      " 0.52663269 0.17239724 0.08149052 0.06183057 0.85649778 0.3860707\n",
      " 0.42002768 0.92116861 0.09373678 0.89048663 0.14953417 0.08612666\n",
      " 0.13651142 0.52794481 0.01484004 0.66441054 0.16030943 0.07422436\n",
      " 0.76547851 0.66740103 0.08973351 0.11385948 0.02777556 0.32617071\n",
      " 0.16041572 0.13703117 0.63349235 0.2885791  0.17616543 0.33648097\n",
      " 0.20567789 0.12159998 0.14017517 0.09878138 0.07723414 0.52180655\n",
      " 0.66862695 0.48808269 0.21905777 0.1970319  0.02091925 0.22130292\n",
      " 0.06661425 0.67947248 0.25473308 0.05884111 0.03510097 0.09777245\n",
      " 0.15764362 0.14903776 0.30365496 0.38537618 0.27056071 0.34997671\n",
      " 0.21281734 0.59883105 0.08192889 0.03760465 0.33269259 0.4844073\n",
      " 0.41251453 0.31416329 0.42130234 0.09517264 0.07047967 0.82175889\n",
      " 0.95968999 0.27699718 0.53735778 0.74345099 0.10966189 0.10870833\n",
      " 0.2227625  0.07948566 0.11521001 0.19122906 0.14044631 0.2544611\n",
      " 0.55129945 0.18259395 0.36655977 0.86073206 0.11441452 0.18149772\n",
      " 0.10459695 0.13390404 0.1738465  0.12721162 0.50010768 0.17615869\n",
      " 0.08069943 0.10745523 0.19400934 0.11809128 0.34991057 0.27766408\n",
      " 0.30590479 0.46891573 0.44370174 0.8962207  0.50794362 0.15145608\n",
      " 0.42186326 0.31072927 0.32488615 0.0621423  0.5984434  0.12477194\n",
      " 0.86452052 0.05290245 0.81146591 0.18090074 0.40728625 0.31989686\n",
      " 0.43831672 0.7048449  0.09210771 0.13725205 0.62992779 0.11831745\n",
      " 0.08947533 0.19884857 0.12805176 0.75162499 0.85439782 0.35406781\n",
      " 0.88838401 0.04696724 0.43049604 0.06343208 0.15030911 0.75992945\n",
      " 0.82328442 0.35022168 0.74052821 0.09557862 0.15695667 0.01898611\n",
      " 0.60538902 0.31816009 0.17166015 0.28608959 0.97068264 0.16163288\n",
      " 0.12692409 0.134581   0.11396011 0.25034563 0.35075408 0.06347744\n",
      " 0.38744357 0.09154371 0.13659986 0.10141571 0.14616506 0.4381617\n",
      " 0.14899673 0.13078609 0.46694054 0.03413934 0.10718177 0.35425615\n",
      " 0.4612312  0.29493502 0.15529437 0.42728812 0.39051448 0.75929625\n",
      " 0.43601612 0.0739837  0.05039199 0.24624343 0.29372731 0.22943498\n",
      " 0.10427448 0.47056689 0.05096786 0.51844952 0.53819918 0.23150004\n",
      " 0.66108252 0.97029625 0.64026785 0.75193734 0.33639327 0.12620475\n",
      " 0.50126674 0.25036589 0.24329919 0.58312987 0.79023968 0.10569645\n",
      " 0.09867096 0.71585099 0.35464577 0.84922546 0.53874414 0.11164654\n",
      " 0.30837416 0.09348796 0.01970665 0.77391755 0.19640329 0.34621518\n",
      " 0.10894015 0.30764792 0.19231456 0.12065035 0.28616427 0.5773454\n",
      " 0.30119351 0.83147082 0.48981781 0.57536341 0.04957199 0.26026935\n",
      " 0.5263046  0.13392736 0.34987684 0.67348728 0.2680861  0.37441009\n",
      " 0.78427049 0.66398495 0.10649492 0.13246689 0.11973119 0.23773376\n",
      " 0.72267299 0.20369119 0.40798115 0.32433633 0.74511252 0.16035896\n",
      " 0.10009848 0.90879483 0.79911821 0.22802873 0.18080482 0.23943758\n",
      " 0.06737687 0.19519651 0.36520723 0.31668376 0.13938409 0.2968497\n",
      " 0.21328071 0.30045478 0.40689528 0.08587223 0.22844709 0.23801866\n",
      " 0.82280608 0.13348211 0.13335408 0.1808124  0.14098917 0.13227127\n",
      " 0.17826617 0.21178806 0.75400811 0.18551116 0.10199279 0.67913635\n",
      " 0.93664481 0.30550739 0.63697453 0.35928886 0.80538298 0.55001448\n",
      " 0.65844084 0.25011919 0.11388163 0.59129159 0.70090048 0.49464168\n",
      " 0.45456924 0.29028012 0.15987514 0.86830318 0.13224477 0.91550424\n",
      " 0.09440943 0.2541044  0.31105911 0.17334954 0.28769127 0.07859348]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pleiades/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "prefix = '/Users/pleiades/work/data/diabetes'\n",
    "# os.chdir('/Users/stevenhurwitt/Documents/Blog/Classification')\n",
    "fpath = os.path.join(prefix, 'diabetes.csv')\n",
    "assert os.path.exists(fpath)\n",
    "df = pd.read_csv(fpath, sep=',',header=0)\n",
    "print(df.head())\n",
    "\n",
    "k = 'class'\n",
    "print(\"> unique values of {}: {}\".format(k, df[k].unique()))\n",
    "\n",
    "# creating a dict file  \n",
    "labels = {'tested_negative': 0,'tested_positive': 1} \n",
    "df[k] = [labels[v] for v in df[k]]\n",
    "\n",
    "y = df['class'].values\n",
    "X = df.drop('class', axis=1).values\n",
    "\n",
    "print(\"> values(y): {}\".format(np.unique(y)))\n",
    "print(\"> dim(X): {}, dim(y): {}\".format(X.shape, y.shape))\n",
    "\n",
    "LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(X, y)\n",
    "y_proba = LR.predict_proba(X)\n",
    "print(y_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> dim(R): (50, 3183), dim(T): (50, 796)\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "def read_fold(path, fold, shuffle=False, test_ratio=None, fold_count=5, random_state=None):\n",
    "    if shuffle: \n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        if random_state is None: random_state = int(time.time()+random.randint(1, 1000)+fold)\n",
    "\n",
    "        # aggregate and then reshuffle\n",
    "        dfs = []\n",
    "        train_df = read_csv('%s/validation-%i.csv.gz' % (path, fold), index_col = [0, 1], compression = 'gzip')\n",
    "        test_df = read_csv('%s/predictions-%i.csv.gz' % (path, fold), index_col = [0, 1], compression = 'gzip')\n",
    "        train_size = train_df.shape[0]\n",
    "        test_size = test_df.shape[0]\n",
    "        dfs.extend([train_df, test_df])\n",
    "\n",
    "        # aggregate \n",
    "        df = concat(dfs, axis = 0)\n",
    "\n",
    "        labels = df.index.get_level_values('label').values\n",
    "        # print('... ids: {ids}'.format(ids=df.index.get_level_values('id').values[:100]))\n",
    "        \n",
    "        ### train-test split\n",
    "        if test_ratio is None or test_ratio <= 0: test_ratio = 1/(fold_count+0.0)\n",
    "\n",
    "        train_df, test_df, train_labels, test_labels = train_test_split(df, labels, test_size=test_ratio, \n",
    "            shuffle=True, stratify=labels, random_state=random_state)  # random_state\n",
    "        # assert all(train_df.index.get_level_values('label').values == train_labels)\n",
    "    else:\n",
    "        train_df        = read_csv('%s/validation-%i.csv.gz' % (path, fold), index_col = [0, 1], compression = 'gzip')\n",
    "        test_df         = read_csv('%s/predictions-%i.csv.gz' % (path, fold), index_col = [0, 1], compression = 'gzip')\n",
    "        train_labels    = train_df.index.get_level_values('label').values\n",
    "        test_labels     = test_df.index.get_level_values('label').values\n",
    "    return train_df, train_labels, test_df, test_labels\n",
    "\n",
    "path = '/Users/pleiades/work/data/pf2'\n",
    "train_df, train_labels, test_df, test_labels = read_fold(path, 1)\n",
    "R = train_df.values.T\n",
    "T = test_df.values.T \n",
    "\n",
    "print(\"> dim(R): {}, dim(T): {}\".format(R.shape, T.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> X: [[2.70000e-04 0.00000e+00 1.67009e-01 5.66828e-01 9.10968e-01 8.41993e-01\n",
      "  9.99994e-01 9.93500e-03 1.67000e-04 3.63000e-04]\n",
      " [2.05263e-01 2.64447e-01 9.98612e-01 9.99422e-01 7.12024e-01 9.18862e-01\n",
      "  6.53757e-01 4.60000e-05 4.49200e-03 0.00000e+00]\n",
      " [3.83500e-03 1.16000e-04 5.24148e-01 8.59905e-01 9.94505e-01 8.26940e-02\n",
      "  9.98994e-01 1.63100e-03 3.11000e-04 4.38470e-02]\n",
      " [2.06500e-03 0.00000e+00 9.98423e-01 9.86084e-01 3.34956e-01 9.85210e-01\n",
      "  9.91423e-01 0.00000e+00 2.50000e-04 7.44184e-01]\n",
      " [1.00000e-06 1.00000e-06 8.55388e-01 8.60899e-01 9.79106e-01 7.35852e-01\n",
      "  9.99289e-01 2.79300e-03 8.69100e-03 3.70110e-02]\n",
      " [1.09200e-03 0.00000e+00 9.35765e-01 8.96329e-01 9.97751e-01 1.31276e-01\n",
      "  9.97744e-01 4.72880e-02 1.44000e-04 1.75000e-03]\n",
      " [6.05490e-01 0.00000e+00 8.03895e-01 9.95544e-01 4.97894e-01 5.97560e-02\n",
      "  9.99955e-01 9.16805e-01 1.14500e-03 0.00000e+00]\n",
      " [6.04367e-01 3.61919e-01 2.92312e-01 9.93239e-01 9.72625e-01 1.53160e-02\n",
      "  9.97028e-01 5.14100e-03 2.21900e-03 6.48510e-02]\n",
      " [8.51270e-02 1.28530e-02 9.94243e-01 4.85027e-01 9.20317e-01 9.58220e-02\n",
      "  9.69285e-01 1.30000e-04 1.10000e-05 0.00000e+00]\n",
      " [5.42680e-02 6.25900e-02 9.97204e-01 9.29732e-01 9.86401e-01 3.10000e-03\n",
      "  9.93866e-01 1.82390e-02 7.45700e-03 5.40841e-01]], L: [[0. 0. 0. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 1. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 0. 0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 0. 1. 0. 0. 1.]]\n",
      "> dim(R): (50, 2785), dim(T): (50, 1194)\n"
     ]
    }
   ],
   "source": [
    "# numerical prediction\n",
    "import collections\n",
    "\n",
    "def estimateLabelMatrix(R, L=[], p_th=[], pos_label=1, neg_label=0, ratio_small_class=0.01):\n",
    "    Lh = np.zeros(R.shape).astype(int)\n",
    "    \n",
    "    for i in range(R.shape[0]):  # foreach user/classifeir\n",
    "        cols_pos = R[i] >= p_th[i]\n",
    "        Lh[i, cols_pos] = pos_label\n",
    "\n",
    "    return Lh\n",
    "\n",
    "def correctness_matrix(X, L, p_th, target_label=None): \n",
    "    \"\"\"\n",
    "    Compute a binary matrix in which 1 represents a correct prediction (i.e. TP or TN), \n",
    "    and 0 represents a false prediction (i.e. FP or FN). Predicted labels (Lh) are determined by the given probability threshold (p_th). \n",
    "    \n",
    "    Lh(X, p_th)\n",
    "\n",
    "    Lh are compared with the \"ground truth\" L to determine the correctness. \n",
    "    \"\"\"\n",
    "    Lh = estimateLabelMatrix(X, p_th=p_th)\n",
    "    \n",
    "    if target_label is not None: \n",
    "        return ((Lh == L[None, :]) & (Lh == target_label)).astype(int)\n",
    "    return (Lh == L[None, :]).astype(int), Lh\n",
    "\n",
    "def polarity_matrix(X, L, p_th, reduced_negative=-1, pos_label=1, neg_label=0): \n",
    "    Mc, Lh = correctness_matrix(X, L, p_th)\n",
    "    return to_polarity(Mc), Lh\n",
    "\n",
    "def color_matrix(X, L, p_th, reduced_negative=False, pos_label=1, neg_label=0): \n",
    "    sample_types = ['tp', 'tn'] + ['fp', 'fn']\n",
    "    codes = {'tp': 2, 'tn': 1, 'fp': -2, 'fn': -1, \n",
    "            'unk': 0, 't': 3, 'f': -3}\n",
    "\n",
    "    Mc, Lh = correctness_matrix(X, L, p_th)  # Mc is a (0, 1)-matrix\n",
    "    n_users = X.shape[0]\n",
    "\n",
    "    predict_pos = (Lh == pos_label)  # Given BP's prediction Lh, select entries ~ target label\n",
    "    predict_neg = (Lh == neg_label)\n",
    "\n",
    "    cells_tp = (Mc == pos_label) & predict_pos   # estimated\n",
    "    cells_tn = (Mc == pos_label) & predict_neg\n",
    "    cells_fp = (Mc == neg_label) & predict_pos\n",
    "    cells_fn = (Mc == neg_label) & predict_neg\n",
    "\n",
    "    Pc = np.zeros(X.shape)\n",
    "    Pc[cells_tp] = codes['tp']\n",
    "    Pc[cells_tn] = codes['tn']\n",
    "\n",
    "    if reduced_negative: \n",
    "        Pc[cells_fp | cells_fn] = -1\n",
    "    else: \n",
    "        Pc[cells_fp] = codes['fp']\n",
    "        Pc[cells_fn] = codes['fn']\n",
    "\n",
    "    return Pc, Lh\n",
    "\n",
    "def to_polarity(M, verify=False): \n",
    "    # from preference matrix to polarity matrix\n",
    "\n",
    "    # if verify: \n",
    "    #     vmin, vmax = np.min(M), np.max(M)\n",
    "\n",
    "    import scipy.sparse as sparse\n",
    "    P = np.ones(M.shape)  \n",
    "    if sparse.issparse(M):      \n",
    "        P[M.toarray() == 0] = -1    # preference 0 ~ negative polarity \n",
    "        P = sparse.csr_matrix(P)\n",
    "    else: \n",
    "        P[M == 0] = -1 \n",
    "    return P\n",
    "\n",
    "pos_label, neg_label = 1, 0 \n",
    "\n",
    "X = np.hstack([R, T]) # np.random.random(dim)\n",
    "Nu, Ni = X.shape \n",
    "dim = (Nu, Ni)\n",
    "\n",
    "Lh = np.zeros(X.shape)\n",
    "Lh[X>=0.5] = 1\n",
    "\n",
    "p_th = [0.5] * X.shape[0]\n",
    "\n",
    "# create labels \n",
    "L = np.zeros(Lh.shape[1])\n",
    "for j in range(X.shape[1]): \n",
    "    # majority vote of the user/classifiers for data j is True => positive\n",
    "    if collections.Counter(X[:, j] >= p_th).most_common(1)[0][0]: # if it's that majory vote says positive\n",
    "         L[j] = pos_label\n",
    "\n",
    "print(\"> X: {}, L: {}\".format(X[:10, :10], Lh[:10, :10]))\n",
    "\n",
    "n_train = int(0.7 * Ni)\n",
    "Lr, Lt = L[:n_train], L[n_train:]\n",
    "y_train, y_test = Lr, Lt    # <<< \n",
    "\n",
    "Mc, Lh = correctness_matrix(X, L, p_th)\n",
    "Lhr, Lht = Lh[:, :n_train], Lh[:, n_train:]\n",
    "\n",
    "R, T = X[:, :n_train], X[:, n_train:]\n",
    "Mcr, Mct = Mc[:, :n_train], Mc[:, n_train:]\n",
    "assert Lhr.shape == R.shape\n",
    "assert Lht.shape == T.shape\n",
    "assert Mcr.shape == R.shape, \"dim(Mcr): {}, dim(R): {}\".format(Mcr.shape, R.shape)\n",
    "print('> dim(R): {}, dim(T): {}'.format(R.shape, T.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Polarity(object): \n",
    "    sample_types = ['tp', 'tn'] + ['fp', 'fn']\n",
    "    codes = {'tp': 2, 'tn': 1, 'fp': -2, 'fn': -1, \n",
    "            'unk': 0, 't': 3, 'f': -3}\n",
    "\n",
    "sample_types = ['tp', 'tn'] + ['fp', 'fn']\n",
    "codes = {'tp': 2, 'tn': 1, 'fp': -2, 'fn': -1, \n",
    "        'unk': 0, 't': 3, 'f': -3}\n",
    "\n",
    "predict_pos = (Lhr == pos_label)  # Given BP's prediction Lh, select entries ~ target label\n",
    "predict_neg = (Lhr == neg_label)\n",
    "cells_tp = (Mcr == 1) & predict_pos   # estimated\n",
    "cells_tn = (Mcr == 1) & predict_neg\n",
    "cells_fp = (Mcr == 0) & predict_pos\n",
    "cells_fn = (Mcr == 0) & predict_neg\n",
    "\n",
    "scopes = {st: {} for st in sample_types}   # scope['tp'][0]: to be true positive, 0th classifier must have this proba range\n",
    "for i in range(R.shape[0]):  # foreach classifier\n",
    "    scopes['tp'][i] = {}\n",
    "\n",
    "    # TPs\n",
    "    v = R[i, :][cells_tp[i, :]]\n",
    "    if len(v) > 0: \n",
    "        scopes['tp'][i] = {'min': np.min(v), 'max': np.max(v), 'mean': np.mean(v), 'median': np.median(v), 'sample': np.sort(v)}   # min, max, mean, median\n",
    "\n",
    "    # TNs \n",
    "    v2 = R[i, :][cells_tn[i, :]]\n",
    "    if len(v2) > 0: \n",
    "        scopes['tn'][i] = {'min': np.min(v2), 'max': np.max(v2), 'mean': np.mean(v2), 'median': np.median(v2), 'sample': np.sort(v)}   \n",
    "\n",
    "    # ... positive polarity candidates \n",
    "    assert scopes['tp'][i]['median'] != scopes['tn'][i]['median'] \n",
    "\n",
    "    # FPs ~ TPs\n",
    "    v3 = R[i, :][cells_fp[i, :]]\n",
    "    if len(v3) > 0: \n",
    "        scopes['fp'][i] = {'min': np.min(v3), 'max': np.max(v3), 'mean': np.mean(v3), 'median': np.median(v3), 'sample': np.sort(v)}\n",
    "\n",
    "    # FNs ~ TNs\n",
    "    v4 = R[i, :][cells_fn[i, :]]\n",
    "    if len(v4) > 0: \n",
    "        scopes['fn'][i] = {'min': np.min(v4), 'max': np.max(v4), 'mean': np.mean(v4), 'median': np.median(v4), 'sample': np.sort(v)}   \n",
    "    # ... negative polarity candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_sequence(R, j, p_th, Rm=None, C=None, Lh=None, p_model={}, name='', index=0, verbose=False, wsize=20): \n",
    "    Nu, Ni = R.shape\n",
    "    fdx = []\n",
    "    for i in range(Nu):  # foreach user/classifier index while holding item index fixed \n",
    "        # one feature dictionary per entry\n",
    "        fd = get_vars_hstats(R, i, j, p_th=p_th, Rm=Rm, C=C, Lh=Lh, p_model=p_model, name=name, wsize=wsize, verbose=verbose, index=index, to_dict=True) \n",
    "        # fdv = get_vars_vstats(R, i, j, p_th=p_th, Rm=Rm, C=C, Lh=Lh, name=name, wsize=wsize, verbose=verbose, index=index, to_dict=True)\n",
    "        # fd.update(fdv) # merge two dictionaries \n",
    "\n",
    "        fdx.append(fd)\n",
    "\n",
    "    # output: a list of feature dictionaries, one per entry/classifier/user while holding column(j) fixed\n",
    "    return fdx    \n",
    "\n",
    "def get_vars_hstats(R, i, j, p_th, Rm=None, C=None, Lh=None, p_model={}, r_min=0.1, name='', index=0, verbose=False, wsize=20, to_dict=False):  \n",
    "    # get BP prediction vector statistics as variables\n",
    "    from scipy.stats import kurtosis, skew, ks_2samp\n",
    "\n",
    "    # sample_types = ['tp', 'tn'] + ['fp', 'fn']\n",
    "    # codes = {'tp': 2, 'tn': 1, 'fp': -2, 'fn': -1, \n",
    "    #         'unk': 0, 't': 3, 'f': -3}\n",
    "    sample_types = Polarity.sample_types\n",
    "    codes = Polarity.codes\n",
    "\n",
    "    msg = \"\"\n",
    "    # query point \n",
    "    q = pt_q = R[i, j]   # q\n",
    "\n",
    "    fv = []  # features \n",
    "    fvn = []  # feature names\n",
    "    \n",
    "    # values \n",
    "    fv.append(q)\n",
    "    fvn.append('value')\n",
    "    \n",
    "    # origina data index \n",
    "#     fv.append(j)\n",
    "#     fvn.append('index')\n",
    "\n",
    "    max_gap = 1.0\n",
    "\n",
    "    N = R.shape[1]\n",
    "    rk = -1   # rank of the query point\n",
    "    \n",
    "    wsize_min, wsize_max = N//100, 20 \n",
    "    wsize = min(wsize_max, max(wsize_min, wsize))\n",
    "\n",
    "    vn = 'delta_pth'\n",
    "    delta = pt_q - p_th[i]\n",
    "    # fv.append( delta ); fvn.append(vn)\n",
    "    # ... case q > p_th, likely TP if L = 1, or FP if L = 0\n",
    "    # ... case q <= p_th, likely FN if L = 1, or TN if L = 0\n",
    "\n",
    "    ### rank?  can also use q-\n",
    "    if Rm is not None and Lh is not None:\n",
    "        vn = 'rank'   # label-specific rank\n",
    "\n",
    "        if delta >= 0: \n",
    "            pts = Rm[i, :][Lh[i, :] == pos_label]\n",
    "            rk = np.searchsorted(pts, pt_q, side='left')\n",
    "        else: \n",
    "            # negative rank\n",
    "            pts = Rm[i, :][Lh[i, :] == neg_label]\n",
    "            n_pts = len(pts)\n",
    "            rkc = np.searchsorted(pts, pt_q, side='left')\n",
    "            rk = -((n_pts+1)-rkc)\n",
    "\n",
    "        # N = Rm.shape[1]\n",
    "        # Rm: either a sorted array (or a rank array)\n",
    "        # r = np.searchsorted(R[i, :], q, side='left')  \n",
    "        fv.append( rk ); fvn.append(vn) \n",
    "\n",
    "    # --- (raw) confidence score ---\n",
    "    vn = 'c-score'\n",
    "    if C is not None: \n",
    "        fv.append(C[i, j])\n",
    "        fvn.append(vn)\n",
    "\n",
    "    # assert len(fv.shape) == 1\n",
    "    assert len(fv) == len(fvn), \"dim(fv): {} <> dim(fvn): {}\".format(len(fv), len(fvn))\n",
    "    if verbose: \n",
    "        # for vn, v in zip(fvn, fv): \n",
    "        msg += \"(get_vars_hstats) vars name values ({}):\\n... {}\\n\".format(name, list(zip(fvn, fv)))\n",
    "        # print(\"... q: {}, topk_th: {} | r_min: {}\".format(q, topk_th, r_min))   # ... ok\n",
    "        print(msg)\n",
    "\n",
    "    if to_dict: \n",
    "        return dict(zip(fvn, fv))\n",
    "\n",
    "    return np.array(fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> neg_sample: [1207 1142 1809 1283 1647 1547 2031 1258 1058 2523]\n",
      "> pos_sample: [ 2  3  4  6 15 20 23 24 28 32]\n"
     ]
    }
   ],
   "source": [
    "# generate features \n",
    "def numeric_to_str(labels, codes={}): \n",
    "    if len(codes) == 0: codes = Polarity.codes\n",
    "    \n",
    "    # inverse the codes \n",
    "    inv_codes = {num: stype for stype, num in codes.items()}\n",
    "    return [inv_codes[l] for i, l in enumerate(labels)]\n",
    "def str_to_numeric(labels, codes={}): # \n",
    "    if len(codes) == 0: codes = Polarity.codes\n",
    "    return [codes[l] for i, l in enumerate(labels)]\n",
    "    \n",
    "def to_numeric(Yh, codes={}): \n",
    "    if len(codes) == 0: codes = Polarity.codes\n",
    "    \n",
    "    Yhn = []\n",
    "    for y in Yh: # foreach label sequence/list\n",
    "        Yhn.append( [codes[e] for e in y] )\n",
    "    return Yhn\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "tMulticlass = True\n",
    "gamma = 2\n",
    "\n",
    "n_users = nbp = R.shape[0]\n",
    "n_train = n_items = R.shape[1]\n",
    "Rs = np.sort(R, axis=1)\n",
    "\n",
    "# subsample the negative to match sample size of the positive\n",
    "Lr = L[:n_train]\n",
    "Mcr, Lhr = correctness_matrix(R, Lr, p_th)\n",
    "\n",
    "pos_sample = np.where(Lr == pos_label)[0]  # <<< \n",
    "neg_sample = np.where(Lr == neg_label)[0]\n",
    "n_neg = len(neg_sample)\n",
    "n_pos = len(pos_sample)\n",
    "\n",
    "neg_sample = np.random.choice(neg_sample, min(n_pos * gamma, n_neg), replace=False)\n",
    "# neg_sample = neg_sample[:n_pos * gamma]\n",
    "\n",
    "print(\"> neg_sample: {}\".format(neg_sample[:10]))\n",
    "print(\"> pos_sample: {}\".format(pos_sample[:10]))\n",
    "\n",
    "Xset, yset = [], []\n",
    "iset = []\n",
    "    \n",
    "for j in neg_sample: # foreach negative-class example \n",
    "\n",
    "    # get feature repr for j-th item by varying user (i) while holding item (j) fixed\n",
    "    fseq = get_feature_sequence(R, j, p_th, Rm=Rs, Lh=Lhr, p_model=scopes, name='TN-FP-seq', verbose=False, wsize=20) # feature sequence\n",
    "    # ... a list of feature dictionaries\n",
    "    Xset.append(fseq)\n",
    "\n",
    "    # each element of the label sequence is either a TN or an FP\n",
    "    ls_j = []  # label sequence for j-th item\n",
    "    for i in range(n_users):  # foreach user/classifeir\n",
    "        if Mcr[i, j] == 1: \n",
    "            polarity = 'pos'\n",
    "            label = 'tn' #  codes['tn'] if tMulticlass else polarity\n",
    "        elif Mcr[i, j] == 0: \n",
    "            polarity = 'neg'\n",
    "            label = 'fp'  # codes['fp'] if tMulticlass else polarity\n",
    "        else: \n",
    "            raise ValueError\n",
    "        ls_j.append(label)\n",
    "\n",
    "    assert len(fseq) == len(ls_j)\n",
    "    yset.append(ls_j)\n",
    "    iset.append(j)\n",
    "\n",
    "#########################\n",
    "\n",
    "for j in pos_sample:  # foreach positive-class example\n",
    "\n",
    "    fseq = get_feature_sequence(R, j, p_th, Rm=Rs, Lh=Lhr, p_model=scopes, name='TP-FN-seq', verbose=False, wsize=20) # feature sequence\n",
    "    Xset.append(fseq)\n",
    "\n",
    "    # each element of the label sequence is either a TP or an FN\n",
    "    ls_j = []  # label sequence for j-th item\n",
    "    for i in range(n_users):  # foreach user/classifeir\n",
    "        if Mcr[i, j] == 1: \n",
    "            polarity = 'pos'\n",
    "            label = 'tp' # codes['tp'] if tMulticlass else polarity\n",
    "        elif Mcr[i, j] == 0: \n",
    "            polarity = 'neg'\n",
    "            label = 'fn' # codes['fn'] if tMulticlass else polarity\n",
    "        else: \n",
    "            raise ValueError\n",
    "        ls_j.append(label)\n",
    "\n",
    "    assert len(fseq) == len(ls_j)\n",
    "    yset.append(ls_j)\n",
    "    iset.append(j)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> prior to standardizing ...\n",
      "> Xset[0][0]: {'value': 0.017227000000000003, 'rank': -1200}\n",
      "> Xset[j]: [{'value': 0.149773, 'rank': -949}, {'value': 0.925999, 'rank': 972}, {'value': 0.0, 'rank': -1658}, {'value': 0.99724, 'rank': 1074}, {'value': 0.988092, 'rank': 994}, {'value': 0.958353, 'rank': 949}, {'value': 0.940753, 'rank': 1024}, {'value': 0.097081, 'rank': -960}, {'value': 0.534, 'rank': 700}, {'value': 0.000637, 'rank': -1390}, {'value': 1.0, 'rank': 815}, {'value': 1.0, 'rank': 771}, {'value': 0.0, 'rank': -1744}, {'value': 0.0, 'rank': -1813}, {'value': 0.0, 'rank': -1740}, {'value': 0.0, 'rank': -1732}, {'value': 0.0, 'rank': -1701}, {'value': 0.0, 'rank': -1740}, {'value': 0.0, 'rank': -1723}, {'value': 0.0, 'rank': -1777}, {'value': 0.988338, 'rank': 962}, {'value': 0.172652, 'rank': -988}, {'value': 0.006876999999999999, 'rank': -1492}, {'value': 0.185341, 'rank': -903}, {'value': 0.5168470000000001, 'rank': 636}, {'value': 0.185975, 'rank': -974}, {'value': 0.096616, 'rank': -1158}, {'value': 0.348389, 'rank': -774}, {'value': 0.171347, 'rank': -1003}, {'value': 0.019043, 'rank': -1421}, {'value': 0.757668, 'rank': 1145}, {'value': 0.594464, 'rank': 784}, {'value': 0.14366600000000002, 'rank': -1514}, {'value': 0.527506, 'rank': 697}, {'value': 0.752891, 'rank': 1060}, {'value': 0.5010140000000001, 'rank': 707}, {'value': 0.328981, 'rank': -1109}, {'value': 0.739937, 'rank': 1085}, {'value': 0.685373, 'rank': 928}, {'value': 0.480049, 'rank': -721}, {'value': 0.4, 'rank': -1236}, {'value': 0.41, 'rank': -1175}, {'value': 0.22399999999999998, 'rank': -2193}, {'value': 0.36, 'rank': -1553}, {'value': 0.396, 'rank': -1296}, {'value': 0.312, 'rank': -1911}, {'value': 0.40399999999999997, 'rank': -1264}, {'value': 0.326, 'rank': -1771}, {'value': 0.37200000000000005, 'rank': -1511}, {'value': 0.306, 'rank': -1960}], R[j_eff]: [1.49773e-01 9.25999e-01 0.00000e+00 9.97240e-01 9.88092e-01 9.58353e-01\n",
      " 9.40753e-01 9.70810e-02 5.34000e-01 6.37000e-04 1.00000e+00 1.00000e+00\n",
      " 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00 9.88338e-01 1.72652e-01 6.87700e-03 1.85341e-01\n",
      " 5.16847e-01 1.85975e-01 9.66160e-02 3.48389e-01 1.71347e-01 1.90430e-02\n",
      " 7.57668e-01 5.94464e-01 1.43666e-01 5.27506e-01 7.52891e-01 5.01014e-01\n",
      " 3.28981e-01 7.39937e-01 6.85373e-01 4.80049e-01 4.00000e-01 4.10000e-01\n",
      " 2.24000e-01 3.60000e-01 3.96000e-01 3.12000e-01 4.04000e-01 3.26000e-01\n",
      " 3.72000e-01 3.06000e-01] | j_eff: 136\n",
      "> yset[j]: ['tn', 'fp', 'tn', 'fp', 'fp', 'fp', 'fp', 'tn', 'fp', 'tn', 'fp', 'fp', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'fp', 'tn', 'tn', 'tn', 'fp', 'tn', 'tn', 'tn', 'tn', 'tn', 'fp', 'fp', 'tn', 'fp', 'fp', 'fp', 'tn', 'fp', 'fp', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn']\n",
      "> polarity: ['tn', 'fp', 'tn', 'fp', 'fp', 'fp', 'fp', 'tn', 'fp', 'tn', 'fp', 'fp', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'fp', 'tn', 'tn', 'tn', 'fp', 'tn', 'tn', 'tn', 'tn', 'tn', 'fp', 'fp', 'tn', 'fp', 'fp', 'fp', 'tn', 'fp', 'fp', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn', 'tn']\n",
      "> polarity: [1, -2, 1, -2, -2, -2, -2, 1, -2, 1, -2, -2, 1, 1, 1, 1, 1, 1, 1, 1, -2, 1, 1, 1, -2, 1, 1, 1, 1, 1, -2, -2, 1, -2, -2, -2, 1, -2, -2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] =?= [ 1. -2.  1. -2. -2. -2. -2.  1. -2.  1. -2. -2.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1. -2.  1.  1.  1. -2.  1.  1.  1.  1.  1. -2. -2.  1. -2. -2. -2.\n",
      "  1. -2. -2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# now train a sequence classifier\n",
    "model = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "print(\"> prior to standardizing ...\")\n",
    "print(\"> Xset[0][0]: {}\".format(Xset[0][0]))\n",
    "\n",
    "j = 192\n",
    "j_eff = iset[j] # Xset[j][0]['index']\n",
    "\n",
    "# j-th row vector corresponds to ieff-th column originally\n",
    "print(\"> Xset[j]: {}, R[j_eff]: {} | j_eff: {}\".format(Xset[j], R[:, j_eff], j_eff))\n",
    "print(\"> yset[j]: {}\".format(yset[j]))\n",
    "\n",
    "Pc, Lhr2 = color_matrix(R, Lr, p_th)\n",
    "assert np.all(Lhr == Lhr2)\n",
    "\n",
    "sl = numeric_to_str(Pc[:, j_eff])\n",
    "print(\"> polarity: {}\".format(sl)) \n",
    "print(\"> polarity: {} =?= {}\".format( str_to_numeric(sl), Pc[:, j_eff]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> labels: ['tn', 'fp', 'fn', 'tp']\n"
     ]
    }
   ],
   "source": [
    "msg = ''\n",
    "\n",
    "Xset, scaler = standardize(Xset)\n",
    "\n",
    "model.fit(Xset, yset) # remember to take transpose\n",
    "polarity_labels = list(model.classes_)\n",
    "print(\"> labels: {}\".format(polarity_labels)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(polarity_modeling) flat F1 score on T: 0.3940633926196925\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pleiades/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Mc, Lh = correctness_matrix(X, L, p_th)\n",
    "Lhr, Lht = Lh[:, :n_train], Lh[:, n_train:]\n",
    "Mcr, Mct = Mc[:, :n_train], Mc[:, n_train:]\n",
    "\n",
    "T = np.zeros(T.shape)\n",
    "Ts = np.sort(T, axis=1)\n",
    "Xset, yset = [], []\n",
    "for j in range(T.shape[1]):\n",
    "\n",
    "    # get feature repr for j-th item by varying user (i) while holding item (j) fixed\n",
    "    fseq = get_feature_sequence(T, j, p_th, Rm=Ts, Lh=Lht, p_model=scopes, name='predict-T', verbose=False, wsize=20) # feature sequence\n",
    "    # ... a list of feature dictionaries\n",
    "    Xset.append(fseq)  # Xset[j] -> feature sequence for the j-th column/item\n",
    "\n",
    "    # [test]\n",
    "    ls_j = []  # label sequence for j-th item\n",
    "    for i in range(n_users):  # foreach user/classifeir\n",
    "        ################################################################################\n",
    "        if Lht[i, j] == pos_label and Mct[i, j] == 1: \n",
    "            ls_j.append('tp' if tMulticlass else '+' )\n",
    "        elif Lht[i, j] == pos_label and Mct[i, j] == 0:\n",
    "            ls_j.append('fp' if tMulticlass else '-' )  # 'fp'\n",
    "        elif Lht[i, j] == neg_label and Mct[i, j] == 1:\n",
    "            ls_j.append('tn' if tMulticlass else '+'  )\n",
    "        elif Lht[i, j] == neg_label and Mct[i, j] == 0:\n",
    "            ls_j.append('fn' if tMulticlass else '-' ) # 'fn'\n",
    "        ################################################################################\n",
    "    # [test]\n",
    "    assert len(fseq) == len(ls_j) == T.shape[0], \\\n",
    "        \"size(feature seq): {}, size(label seq): {}, n_classifiers: {}\".format(len(fseq), len(ls_j), T.shape[0])\n",
    "    yset.append(ls_j)\n",
    "\n",
    "Xset = transform(Xset, scaler)\n",
    "y_pred = model.predict( Xset )  # transform(Xset, scaler)\n",
    "\n",
    "# convert to np.array format \n",
    "M = np.zeros(T.shape)\n",
    "for j, yj in enumerate(y_pred): \n",
    "    if j == 0: assert len(yj) == T.shape[0]\n",
    "        \n",
    "    # yj: sequence/list of strings as labels\n",
    "    M[:, j] = str_to_numeric(yj)\n",
    "\n",
    "f1 = metrics.flat_f1_score(yset, y_pred, \n",
    "              average='weighted', labels=polarity_labels)\n",
    "msg += \"(polarity_modeling) flat F1 score on T: {}\\n\".format(f1)\n",
    "\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = yset\n",
    "\n",
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    polarity_labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model selection\n",
    "\n",
    "def optimize_crf_params(X, y, model, labels, max_size=5000): \n",
    "    import scipy.stats as stats\n",
    "    from sklearn.metrics import make_scorer\n",
    "    # import sklearn_crfsuite\n",
    "    from sklearn_crfsuite import scorers\n",
    "    from sklearn_crfsuite import metrics\n",
    "\n",
    "    params_space = {\n",
    "        'c1': stats.expon(scale=0.5),\n",
    "        'c2': stats.expon(scale=0.05),\n",
    "    }\n",
    "\n",
    "    # use f1 for evaluation\n",
    "    f1_scorer = make_scorer(metrics.flat_f1_score, \n",
    "                            average='weighted', labels=labels)\n",
    "\n",
    "    # search\n",
    "    rs = RandomizedSearchCV(model, params_space, \n",
    "                            cv=3, \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1, \n",
    "                            n_iter=50, \n",
    "                            scoring=f1_scorer)\n",
    "\n",
    "    X_train, y_train = X, y \n",
    "    N = len(X_train)\n",
    "    if N > max_size: \n",
    "        indices = np.random.choice(range(N), max_size)\n",
    "        X_train = list( np.asarray(X_train)[indices] )\n",
    "        y_train = list( np.asarray(y_train)[indices] )\n",
    "\n",
    "    rs.fit(X_train, y_train)\n",
    "\n",
    "    print('best params:', rs.best_params_)\n",
    "    print('best CV score:', rs.best_score_)\n",
    "    print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "\n",
    "#     validate_crf_params(rs, output_path=None, dpi=300)\n",
    "\n",
    "    return rs.best_estimator_\n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xset = transform(Xset, scaler)\n",
    "\n",
    "model = optimize_crf_params(Xset, yset, model, labels=polarity_labels, max_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = model.best_estimator_\n",
    "X_test, y_test = Xset, yset\n",
    "y_pred = model.predict( X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))\n",
    "\n",
    "f1 = metrics.flat_f1_score(y_test, y_pred, \n",
    "              average='weighted', labels=polarity_labels)\n",
    "msg += \"(polarity_modeling) flat F1 score on T: {} AFTER model selection\\n\".format(f1)\n",
    "\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
