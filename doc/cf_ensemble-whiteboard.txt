

preditNewItems => predict_new_items()

fast_similarity => pairwise_similarity()


evaCrossSimilarity => eval_cross_similarity

# Instead of using Cw (a dense matrix including weights for FPs, FNs), use Cn (masked confidence matrix)
Xc, yc, weights, colors = dp.matrix_to_augmented_training_data(X, Cn, Pc) # NOTE: Don't overwrite X (`Xc` is not the same as `X`, which is a rating matrix)
yc = np.column_stack([yc, weights, colors])

test_size = 0.1
split_pt = int((1-test_size) * Xc.shape[0])
X_train, X_val, y_train, y_val = (
    Xc[:split_pt],
    Xc[split_pt:],
    yc[:split_pt],
    yc[split_pt:])

loss_fn = cm.confidence_weighted_loss
model = cm.get_cfnet_uncompiled(n_users, n_items, n_factors)
model.compile(loss=loss_fn, optimizer=keras.optimizers.Adam(lr=0.001))

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="logs")
history = model.fit(
    x=X_train,
    y=y_train,
    # sample_weight=W_train, # not using sample weight in this case
    batch_size=64,
    epochs=100,
    verbose=1,
    validation_data=(X_val, y_val), # test how the model predict unseen ratings
    callbacks=[tensorboard_callback]
)

%matplotlib inline
f, ax1 = plt.subplots(nrows=1, ncols=1,figsize=(20,8))

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

%load_ext tensorboard
%tensorboard --logdir logs

highlight("(C-Sqr) Reestimate the entire rating matrix (X) with learned latent factors/embeddings")
analyze_reconstruction(reestimate_unreliable_only=False)
highlight("(C-Sqr) Reestimate ONLY the unreliable entries in X with learned latent factors/embeddings")
analyze_reconstruction(reestimate_unreliable_only=True)



### Todo ### 


# cf2.py, data_pipeline.py 
- reestimate 




> the use of sample_weights seems to be helpful (bias: 0.97)
  : use dp.matrix_to_augmented_training_data(X, C0, P)
  >> delta(T, Th) becomes larger 
  
 
 [info] From R to Rh, delta(Frobenius norm)= 37.145144646099126
[info] From T to Th, delta(Frobenius norm)= 19.018623044095637
[info] How different are lh and lh_new? 0.0544
[result] F1 score with the original T:  0.24657534246575344
[result] F1 score with re-estimated Th: 0.29464285714285715

> if classes too skewed 0.99, then Non-linear CF does not seem to have an advantage

even with sample weights, still not helpful 
[info] From R to Rh, delta(Frobenius norm)= 29.272192971677548
[info] From T to Th, delta(Frobenius norm)= 14.92889392268832
[info] How different are lh and lh_new? 0.2104
[result] F1 score with the original T:  0.2131979695431472
[result] F1 score with re-estimated Th: 0.1834061135371179


(estimateProbThresholds) policy: fmax
[info] From R to Rh, delta(Frobenius norm)= 27.61871649253301
[info] From T to Th, delta(Frobenius norm)= 14.831586213098268
[info] How different are lh and lh_new? 0.1752
[result] F1 score with the original T:  0.18719211822660098
[result] F1 score with re-estimated Th: 0.1818181818181818


# Why is Rh worse than Th? See the log at analyze_reconstruction() ... 
> (reconstruct) Quality of Rh | fmax(Rbase):



# 
df_to_rating_matrix()

# legacy code: vary hard to work with

mfb_ensemble() -> evalTestSet



# balance classe weights 
TP TN FP FN 
3  10 1  6 

T: 4 
F: 16 

total=20 

20/3, 20/10, 20/1, 20/6


# probability filter, confidence matrix

# polarity matrix, color matrix 
polarity: {1, -1}, -1 for FP, FN and 1 for TP, TN


# Cascade mode 
  - Consider R and T i.e. the probability matrix associated with the training and test split

# Parameters
 
  - conf_measure
        'uniform' 

# confidence matrix 
  
  evalConfidenceMatrix()
     toConfidenceMatrix()
     
     estimateLabels()
     
     make_over()
     
  correctness matrix 
  
  color matrix  
     
     
# Confidence scoring matrix

  confidence2D()
      confidence_pointwise_ensemble_prediction() 
      condidence_brier()
     
     
wmf_ensemble_suite_multimodel() 

test_wmf_probs_suite()

   wmf_ensemble_suite() 
       wmf_ensemble( )
          wmf_ensemble_iter2()
          


# Guesstimate the probability filter matrix (mask) based on statistics in the training data 

- for each classifier, do kernel density estimate (KDE) on its probability vectors
  <challenges> 
     - ks cannot differentiate TPs from FPs (their distributions are too similar under KS statistic) 
     - relative skew, relative kurtosis (wrt the knn of the query point) seem different
     ... note [1]

polarity_modeling()
  polarity_feature_extraction()
  
  get_vars_hstats()
  
estimateLabels()



# Compute Probability Filter matrix

mask_over() 
   filter_along_item_axis()
   
   filter_along_user_axis()
   
   
C: confidence matrix

# probability filter matrix /mask
  - probability matrix => threshold (fmax) 
  - given p_th and labels => filter (training set)
  - for test set, more involved

# Confidence Matrix 
  - elementwise product between confidence scores (each c x d has a score) and the filter

toConfidenceMatrix()  # <<< 

Note
----
[1]     * *ks cannot differentiate TPs from FPs (their distributions are too similar under KS statistic)*
    * *however, relative skew, relative kurtosis (wrt the knn of the query point) seem different*
        * FP seems to have larger absolute relative skew and kurtosis